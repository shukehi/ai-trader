#!/usr/bin/env python3
"""
å®æ—¶åˆ†æå¼•æ“ - åŸºäºWebSocketçš„æ™ºèƒ½å¤šæ—¶é—´å‘¨æœŸå®æ—¶åˆ†æç³»ç»Ÿ
æ•´åˆWebSocketæ•°æ®æµã€å¤šæ—¶é—´æ¡†æ¶åˆ†æå™¨å’ŒåŠ¨æ€åˆ†æé¢‘ç‡æ§åˆ¶
"""

import time
from typing import Dict, List, Any, Optional, Callable
import logging
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
from threading import Lock
import queue
import threading

from .multi_timeframe_analyzer import MultiTimeframeAnalyzer, AnalysisScenario, MultiTimeframeResult
from .analysis_context import AnalysisContext, ContextPriority
from data.binance_websocket import BinanceWebSocketClient, StreamConfig, KlineData, ConnectionState
from data import BinanceFetcher

logger = logging.getLogger(__name__)

class AnalysisFrequency(Enum):
    """åˆ†æé¢‘ç‡çº§åˆ«"""
    REALTIME = "realtime"      # å®æ—¶åˆ†æï¼ˆæ¯ä¸ªKçº¿å®Œæˆï¼‰
    HIGH = "high"              # é«˜é¢‘åˆ†æï¼ˆ5åˆ†é’Ÿï¼‰
    NORMAL = "normal"          # æ ‡å‡†é¢‘ç‡ï¼ˆ15åˆ†é’Ÿï¼‰
    LOW = "low"               # ä½é¢‘åˆ†æï¼ˆ1å°æ—¶ï¼‰
    MANUAL = "manual"         # æ‰‹åŠ¨è§¦å‘

class MarketCondition(Enum):
    """å¸‚åœºçŠ¶å†µ"""
    VOLATILE = "volatile"      # é«˜æ³¢åŠ¨
    TRENDING = "trending"      # è¶‹åŠ¿è¡Œæƒ…
    RANGING = "ranging"        # éœ‡è¡è¡Œæƒ…
    QUIET = "quiet"           # å¹³é™å¸‚åœº

@dataclass
class RealtimeConfig:
    """å®æ—¶åˆ†æé…ç½®"""
    symbol: str = "ETHUSDT"
    timeframes: List[str] = field(default_factory=lambda: ["5m", "15m", "1h", "4h"])
    base_frequency: AnalysisFrequency = AnalysisFrequency.NORMAL
    adaptive_frequency: bool = True
    model: str = "gpt5-chat"
    analysis_type: str = "complete"
    analysis_method: Optional[str] = None
    volatility_threshold: float = 0.02  # 2%æ³¢åŠ¨ç‡é˜ˆå€¼
    volume_threshold: float = 2.0       # æˆäº¤é‡å¼‚å¸¸å€æ•°
    max_analysis_per_hour: int = 20     # æ¯å°æ—¶æœ€å¤§åˆ†ææ¬¡æ•°

@dataclass
class AnalysisEvent:
    """åˆ†æäº‹ä»¶"""
    timestamp: datetime
    trigger_type: str           # è§¦å‘ç±»å‹ï¼škline_complete, volatility_spike, volume_surge, manual
    timeframe: str
    priority: int              # ä¼˜å…ˆçº§ 1-10
    market_condition: MarketCondition
    kline_data: Optional[KlineData] = None

class FrequencyAdaptor:
    """åŠ¨æ€é¢‘ç‡é€‚é…å™¨"""
    
    def __init__(self, config: RealtimeConfig):
        self.config = config
        self.analysis_count = 0
        self.hour_start = datetime.now().hour
        self.recent_analyses = []
        self._lock = Lock()
    
    def should_analyze(self, event: AnalysisEvent) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œåˆ†æ"""
        with self._lock:
            current_hour = datetime.now().hour
            
            # é‡ç½®å°æ—¶è®¡æ•°
            if current_hour != self.hour_start:
                self.analysis_count = 0
                self.hour_start = current_hour
                self.recent_analyses = []
            
            # æ£€æŸ¥é¢‘ç‡é™åˆ¶
            if self.analysis_count >= self.config.max_analysis_per_hour:
                logger.warning(f"âš ï¸ å·²è¾¾åˆ°æ¯å°æ—¶æœ€å¤§åˆ†ææ¬¡æ•°é™åˆ¶: {self.config.max_analysis_per_hour}")
                return False
            
            # åŸºäºé…ç½®çš„é¢‘ç‡æ§åˆ¶
            if not self.config.adaptive_frequency:
                return self._check_base_frequency(event)
            
            # è‡ªé€‚åº”é¢‘ç‡æ§åˆ¶
            return self._check_adaptive_frequency(event)
    
    def _check_base_frequency(self, event: AnalysisEvent) -> bool:
        """æ£€æŸ¥åŸºç¡€é¢‘ç‡"""
        now = datetime.now()
        
        if self.config.base_frequency == AnalysisFrequency.REALTIME:
            return event.trigger_type == "kline_complete"
        elif self.config.base_frequency == AnalysisFrequency.HIGH:
            # 5åˆ†é’Ÿå†…æœ€å¤šä¸€æ¬¡åˆ†æ
            return not any(
                (now - analysis_time).total_seconds() < 300 
                for analysis_time in self.recent_analyses
            )
        elif self.config.base_frequency == AnalysisFrequency.NORMAL:
            # 15åˆ†é’Ÿå†…æœ€å¤šä¸€æ¬¡åˆ†æ
            return not any(
                (now - analysis_time).total_seconds() < 900 
                for analysis_time in self.recent_analyses
            )
        elif self.config.base_frequency == AnalysisFrequency.LOW:
            # 1å°æ—¶å†…æœ€å¤šä¸€æ¬¡åˆ†æ
            return not any(
                (now - analysis_time).total_seconds() < 3600 
                for analysis_time in self.recent_analyses
            )
        else:
            return False
    
    def _check_adaptive_frequency(self, event: AnalysisEvent) -> bool:
        """æ£€æŸ¥è‡ªé€‚åº”é¢‘ç‡"""
        
        # é«˜ä¼˜å…ˆçº§äº‹ä»¶ä¼˜å…ˆå¤„ç†
        if event.priority >= 8:
            logger.info(f"ğŸ”¥ é«˜ä¼˜å…ˆçº§äº‹ä»¶ï¼Œç«‹å³åˆ†æ: {event.trigger_type}")
            return True
        
        # åŸºäºå¸‚åœºçŠ¶å†µè°ƒæ•´é¢‘ç‡
        if event.market_condition == MarketCondition.VOLATILE:
            # é«˜æ³¢åŠ¨æœŸé—´ï¼Œæé«˜åˆ†æé¢‘ç‡
            min_interval = 180  # 3åˆ†é’Ÿ
        elif event.market_condition == MarketCondition.TRENDING:
            # è¶‹åŠ¿æœŸé—´ï¼Œæ­£å¸¸é¢‘ç‡
            min_interval = 600  # 10åˆ†é’Ÿ
        elif event.market_condition == MarketCondition.RANGING:
            # éœ‡è¡æœŸé—´ï¼Œé™ä½é¢‘ç‡
            min_interval = 900  # 15åˆ†é’Ÿ
        else:  # QUIET
            # å¹³é™æœŸé—´ï¼Œå¤§å¹…é™ä½é¢‘ç‡
            min_interval = 1800  # 30åˆ†é’Ÿ
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ—¶é—´é—´éš”
        now = datetime.now()
        return not any(
            (now - analysis_time).total_seconds() < min_interval 
            for analysis_time in self.recent_analyses
        )
    
    def record_analysis(self):
        """è®°å½•åˆ†ææ‰§è¡Œ"""
        with self._lock:
            now = datetime.now()
            self.recent_analyses.append(now)
            self.analysis_count += 1
            
            # ä¿æŒæœ€è¿‘24å°æ—¶çš„è®°å½•
            cutoff = now - timedelta(hours=24)
            self.recent_analyses = [t for t in self.recent_analyses if t > cutoff]

class MarketConditionDetector:
    """å¸‚åœºçŠ¶å†µæ£€æµ‹å™¨"""
    
    def __init__(self):
        self.price_history = []
        self.volume_history = []
        self.max_history = 100
        self._lock = Lock()
    
    def update_data(self, kline: KlineData):
        """æ›´æ–°å¸‚åœºæ•°æ®"""
        with self._lock:
            self.price_history.append({
                'timestamp': kline.close_time,
                'price': kline.close_price,
                'volume': kline.volume
            })
            
            # ä¿æŒå†å²æ•°æ®å¤§å°
            if len(self.price_history) > self.max_history:
                self.price_history.pop(0)
    
    def detect_condition(self, kline: KlineData) -> MarketCondition:
        """æ£€æµ‹å¸‚åœºçŠ¶å†µ"""
        if len(self.price_history) < 10:
            return MarketCondition.QUIET
            
        with self._lock:
            recent_prices = [p['price'] for p in self.price_history[-20:]]
            recent_volumes = [p['volume'] for p in self.price_history[-20:]]
            
            # è®¡ç®—ä»·æ ¼æ³¢åŠ¨ç‡
            price_changes = [
                abs(recent_prices[i] - recent_prices[i-1]) / recent_prices[i-1]
                for i in range(1, len(recent_prices))
            ]
            avg_volatility = sum(price_changes) / len(price_changes) if price_changes else 0
            
            # è®¡ç®—æˆäº¤é‡æ¯”ç‡
            avg_volume = sum(recent_volumes) / len(recent_volumes)
            current_volume_ratio = kline.volume / avg_volume if avg_volume > 0 else 1
            
            # è¶‹åŠ¿æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if len(recent_prices) >= 10:
                trend_strength = self._calculate_trend_strength(recent_prices[-10:])
            else:
                trend_strength = 0
            
            # çŠ¶å†µåˆ¤å®šé€»è¾‘
            if avg_volatility > 0.015 and current_volume_ratio > 1.5:  # é«˜æ³¢åŠ¨ + é«˜æˆäº¤é‡
                return MarketCondition.VOLATILE
            elif abs(trend_strength) > 0.7:  # å¼ºè¶‹åŠ¿
                return MarketCondition.TRENDING
            elif avg_volatility > 0.005:  # ä¸­ç­‰æ³¢åŠ¨
                return MarketCondition.RANGING
            else:
                return MarketCondition.QUIET
    
    def _calculate_trend_strength(self, prices: List[float]) -> float:
        """è®¡ç®—è¶‹åŠ¿å¼ºåº¦ (-1åˆ°1, è´Ÿæ•°ä¸‹è·Œï¼Œæ­£æ•°ä¸Šæ¶¨)"""
        if len(prices) < 3:
            return 0
            
        # ç®€åŒ–çš„è¶‹åŠ¿å¼ºåº¦è®¡ç®—
        start_price = prices[0]
        end_price = prices[-1]
        price_change = (end_price - start_price) / start_price
        
        # è®¡ç®—ä»·æ ¼ä¸€è‡´æ€§ï¼ˆå‡å°‘å™ªéŸ³å½±å“ï¼‰
        direction_consistency = 0
        for i in range(1, len(prices)):
            if price_change > 0 and prices[i] > prices[i-1]:
                direction_consistency += 1
            elif price_change < 0 and prices[i] < prices[i-1]:
                direction_consistency += 1
        
        consistency_ratio = direction_consistency / (len(prices) - 1)
        
        # ç»“åˆä»·æ ¼å˜åŒ–å¹…åº¦å’Œæ–¹å‘ä¸€è‡´æ€§
        trend_strength = price_change * consistency_ratio
        
        # é™åˆ¶åœ¨-1åˆ°1ä¹‹é—´
        return max(-1, min(1, trend_strength * 10))

class RealtimeAnalysisEngine:
    """
    å®æ—¶åˆ†æå¼•æ“
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. WebSocketå®æ—¶æ•°æ®ç›‘å¬å’Œå¤„ç†
    2. æ™ºèƒ½åˆ†æé¢‘ç‡æ§åˆ¶å’Œäº‹ä»¶è§¦å‘
    3. å¤šæ—¶é—´æ¡†æ¶å®æ—¶åˆ†ææ‰§è¡Œ
    4. åˆ†æç»“æœç¼“å­˜å’Œå†å²ç®¡ç†
    5. å¸‚åœºçŠ¶å†µæ£€æµ‹å’Œè‡ªé€‚åº”è°ƒæ•´
    """
    
    def __init__(self, config: Optional[RealtimeConfig] = None, api_key: Optional[str] = None):
        """åˆå§‹åŒ–å®æ—¶åˆ†æå¼•æ“"""
        self.config = config or RealtimeConfig()
        
        # æ ¸å¿ƒç»„ä»¶
        self.multi_analyzer = MultiTimeframeAnalyzer(api_key)
        self.analysis_context = AnalysisContext()
        self.fetcher = BinanceFetcher()
        
        # WebSocketå®¢æˆ·ç«¯
        ws_config = StreamConfig(
            symbol=self.config.symbol,
            timeframes=self.config.timeframes
        )
        self.ws_client = BinanceWebSocketClient(ws_config)
        
        # è¾…åŠ©ç»„ä»¶
        self.frequency_adaptor = FrequencyAdaptor(self.config)
        self.condition_detector = MarketConditionDetector()
        
        # è¿è¡Œæ—¶çŠ¶æ€
        self.is_running = False
        self.analysis_queue = queue.Queue()
        self.current_analysis_task = None
        
        # ç»“æœç¼“å­˜
        self.latest_results = {}
        self.results_lock = Lock()
        
        # äº‹ä»¶å›è°ƒ
        self.analysis_callbacks: List[Callable[[MultiTimeframeResult], None]] = []
        self.error_callbacks: List[Callable[[Exception], None]] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_analyses': 0,
            'successful_analyses': 0,
            'failed_analyses': 0,
            'start_time': None,
            'last_analysis_time': None
        }
        
        # è®¾ç½®WebSocketå›è°ƒ
        self._setup_websocket_callbacks()
        
        logger.info("âœ… å®æ—¶åˆ†æå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š ç›‘æ§é…ç½®: {self.config.symbol}, {self.config.timeframes}")
        logger.info(f"ğŸ”„ åŸºç¡€é¢‘ç‡: {self.config.base_frequency.value}, è‡ªé€‚åº”: {self.config.adaptive_frequency}")
    
    def _setup_websocket_callbacks(self):
        """è®¾ç½®WebSocketå›è°ƒå‡½æ•°"""
        
        async def on_kline_complete(kline: KlineData):
            """Kçº¿å®Œæˆå›è°ƒ"""
            try:
                # æ›´æ–°å¸‚åœºçŠ¶å†µæ£€æµ‹å™¨
                self.condition_detector.update_data(kline)
                market_condition = self.condition_detector.detect_condition(kline)
                
                # åˆ›å»ºåˆ†æäº‹ä»¶
                event = AnalysisEvent(
                    timestamp=kline.close_time,
                    trigger_type="kline_complete",
                    timeframe=kline.timeframe,
                    priority=self._calculate_event_priority(kline, market_condition),
                    market_condition=market_condition,
                    kline_data=kline
                )
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ
                if self.frequency_adaptor.should_analyze(event):
                    logger.info(f"ğŸ“Š è§¦å‘åˆ†æäº‹ä»¶: {kline.symbol} {kline.timeframe} - {market_condition.value}")
                    self.analysis_queue.put(event)
                    
            except Exception as e:
                logger.error(f"âŒ Kçº¿å›è°ƒå¤„ç†å¤±è´¥: {e}")
                self._notify_error_callbacks(e)
        
        def on_connection_state_change(state: ConnectionState):
            """è¿æ¥çŠ¶æ€å˜åŒ–å›è°ƒ"""
            logger.info(f"ğŸ”— WebSocketè¿æ¥çŠ¶æ€: {state.value}")
            
            if state == ConnectionState.CONNECTED:
                self.analysis_context.add_context_event(
                    symbol=self.config.symbol,
                    event_type="websocket_connected",
                    description="WebSocketè¿æ¥å·²å»ºç«‹",
                    priority=ContextPriority.NORMAL
                )
            elif state == ConnectionState.DISCONNECTED:
                self.analysis_context.add_context_event(
                    symbol=self.config.symbol,
                    event_type="websocket_disconnected", 
                    description="WebSocketè¿æ¥å·²æ–­å¼€",
                    priority=ContextPriority.HIGH
                )
        
        # ä¸ºæ‰€æœ‰ç›‘æ§çš„æ—¶é—´æ¡†æ¶æ·»åŠ å›è°ƒ
        for timeframe in self.config.timeframes:
            self.ws_client.add_kline_callback(timeframe, on_kline_complete)
        
        self.ws_client.add_connection_callback(on_connection_state_change)
    
    def _calculate_event_priority(self, kline: KlineData, market_condition: MarketCondition) -> int:
        """è®¡ç®—äº‹ä»¶ä¼˜å…ˆçº§ (1-10)"""
        priority = 5  # åŸºç¡€ä¼˜å…ˆçº§
        
        # åŸºäºå¸‚åœºçŠ¶å†µè°ƒæ•´ä¼˜å…ˆçº§
        if market_condition == MarketCondition.VOLATILE:
            priority += 3
        elif market_condition == MarketCondition.TRENDING:
            priority += 1
        elif market_condition == MarketCondition.QUIET:
            priority -= 2
        
        # åŸºäºæ—¶é—´æ¡†æ¶è°ƒæ•´ä¼˜å…ˆçº§ï¼ˆæ›´é•¿çš„æ—¶é—´æ¡†æ¶ä¼˜å…ˆçº§æ›´é«˜ï¼‰
        timeframe_priorities = {
            '1m': 1, '5m': 2, '15m': 3, '30m': 4,
            '1h': 5, '4h': 7, '1d': 9, '1w': 10
        }
        tf_priority = timeframe_priorities.get(kline.timeframe, 5)
        priority = max(1, min(10, priority + (tf_priority - 5)))
        
        return priority
    
    async def start_realtime_analysis(self):
        """å¯åŠ¨å®æ—¶åˆ†æ"""
        if self.is_running:
            logger.warning("å®æ—¶åˆ†æå·²åœ¨è¿è¡Œä¸­")
            return
            
        try:
            self.is_running = True
            self.stats['start_time'] = datetime.now()
            
            logger.info("ğŸš€ å¯åŠ¨å®æ—¶åˆ†æå¼•æ“")
            
            # å¯åŠ¨åˆ†æå¤„ç†çº¿ç¨‹
            analysis_thread = threading.Thread(target=self._analysis_worker, daemon=True)
            analysis_thread.start()
            
            # è¿æ¥WebSocket
            await self.ws_client.connect()
            
            # å¯åŠ¨WebSocketç›‘å¬å¾ªç¯
            await self.ws_client.listen()
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨å®æ—¶åˆ†æå¤±è´¥: {e}")
            self.is_running = False
            self._notify_error_callbacks(e)
            raise
    
    def _analysis_worker(self):
        """åˆ†æä»»åŠ¡å·¥ä½œçº¿ç¨‹"""
        logger.info("ğŸ“Š åˆ†æå·¥ä½œçº¿ç¨‹å·²å¯åŠ¨")
        
        while self.is_running:
            try:
                # è·å–åˆ†æäº‹ä»¶ï¼ˆé˜»å¡ç­‰å¾…ï¼‰
                try:
                    event = self.analysis_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                # æ‰§è¡Œåˆ†æ
                self._execute_analysis(event)
                self.analysis_queue.task_done()
                
            except Exception as e:
                logger.error(f"âŒ åˆ†æå·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
                self._notify_error_callbacks(e)
    
    def _execute_analysis(self, event: AnalysisEvent):
        """æ‰§è¡Œåˆ†æä»»åŠ¡"""
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ” æ‰§è¡Œåˆ†æ: {self.config.symbol} - è§¦å‘ç±»å‹: {event.trigger_type}")
            
            # è®°å½•åˆ†ææ‰§è¡Œ
            self.frequency_adaptor.record_analysis()
            
            # æ‰§è¡Œå¤šæ—¶é—´æ¡†æ¶åˆ†æ
            result = self.multi_analyzer.analyze_multi_timeframe(
                symbol=self.config.symbol,
                model=self.config.model,
                analysis_type=self.config.analysis_type,
                analysis_method=self.config.analysis_method,
                scenario=self._determine_analysis_scenario(event)
            )
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['total_analyses'] += 1
            self.stats['last_analysis_time'] = datetime.now()
            
            if result.overall_signal != "ERROR":
                self.stats['successful_analyses'] += 1
                
                # ç¼“å­˜æœ€æ–°ç»“æœ
                with self.results_lock:
                    self.latest_results[self.config.symbol] = result
                
                # æ·»åŠ ä¸Šä¸‹æ–‡äº‹ä»¶
                self.analysis_context.add_context_event(
                    symbol=self.config.symbol,
                    event_type="analysis_completed",
                    description=f"å®æ—¶åˆ†æå®Œæˆ - {result.overall_signal}",
                    priority=ContextPriority.NORMAL,
                    metadata={
                        'consistency_score': result.consistency_score,
                        'confidence_level': result.confidence_level,
                        'execution_time': result.execution_time,
                        'trigger_type': event.trigger_type
                    }
                )
                
                # é€šçŸ¥å›è°ƒå‡½æ•°
                self._notify_analysis_callbacks(result)
                
                logger.info(f"âœ… å®æ—¶åˆ†æå®Œæˆ - ä¿¡å·: {result.overall_signal}, è€—æ—¶: {time.time() - start_time:.2f}ç§’")
            else:
                self.stats['failed_analyses'] += 1
                logger.error(f"âŒ åˆ†æå¤±è´¥ - é”™è¯¯: {result.risk_warnings}")
                
        except Exception as e:
            self.stats['failed_analyses'] += 1
            logger.error(f"âŒ æ‰§è¡Œåˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {e}")
            self._notify_error_callbacks(e)
    
    def _determine_analysis_scenario(self, event: AnalysisEvent) -> AnalysisScenario:
        """æ ¹æ®äº‹ä»¶ç¡®å®šåˆ†æåœºæ™¯"""
        if event.market_condition == MarketCondition.VOLATILE:
            return AnalysisScenario.INTRADAY_TRADING
        elif event.market_condition == MarketCondition.TRENDING:
            return AnalysisScenario.TREND_ANALYSIS
        elif event.market_condition == MarketCondition.RANGING:
            return AnalysisScenario.SWING_TRADING
        else:
            return AnalysisScenario.QUICK_CHECK
    
    def trigger_manual_analysis(self) -> bool:
        """æ‰‹åŠ¨è§¦å‘åˆ†æ"""
        try:
            event = AnalysisEvent(
                timestamp=datetime.now(),
                trigger_type="manual",
                timeframe=self.config.timeframes[0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ—¶é—´æ¡†æ¶
                priority=10,  # æœ€é«˜ä¼˜å…ˆçº§
                market_condition=MarketCondition.QUIET  # é»˜è®¤å¸‚åœºçŠ¶å†µ
            )
            
            self.analysis_queue.put(event)
            logger.info("ğŸ–±ï¸ æ‰‹åŠ¨åˆ†æå·²è§¦å‘")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ‰‹åŠ¨è§¦å‘åˆ†æå¤±è´¥: {e}")
            return False
    
    def get_latest_result(self) -> Optional[MultiTimeframeResult]:
        """è·å–æœ€æ–°åˆ†æç»“æœ"""
        with self.results_lock:
            return self.latest_results.get(self.config.symbol)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = dict(self.stats)
        if stats['start_time']:
            stats['running_time'] = (datetime.now() - stats['start_time']).total_seconds()
        stats['success_rate'] = (
            stats['successful_analyses'] / stats['total_analyses'] 
            if stats['total_analyses'] > 0 else 0
        )
        stats['is_running'] = self.is_running
        stats['queue_size'] = self.analysis_queue.qsize()
        return stats
    
    def add_analysis_callback(self, callback: Callable[[MultiTimeframeResult], None]):
        """æ·»åŠ åˆ†æç»“æœå›è°ƒ"""
        self.analysis_callbacks.append(callback)
    
    def add_error_callback(self, callback: Callable[[Exception], None]):
        """æ·»åŠ é”™è¯¯å›è°ƒ"""
        self.error_callbacks.append(callback)
    
    def _notify_analysis_callbacks(self, result: MultiTimeframeResult):
        """é€šçŸ¥åˆ†æå›è°ƒ"""
        for callback in self.analysis_callbacks:
            try:
                callback(result)
            except Exception as e:
                logger.error(f"âŒ åˆ†æå›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
    
    def _notify_error_callbacks(self, error: Exception):
        """é€šçŸ¥é”™è¯¯å›è°ƒ"""
        for callback in self.error_callbacks:
            try:
                callback(error)
            except Exception as e:
                logger.error(f"âŒ é”™è¯¯å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
    
    async def stop(self):
        """åœæ­¢å®æ—¶åˆ†æ"""
        if not self.is_running:
            return
            
        logger.info("ğŸ›‘ æ­£åœ¨åœæ­¢å®æ—¶åˆ†æå¼•æ“")
        
        self.is_running = False
        
        # åœæ­¢WebSocketè¿æ¥
        if self.ws_client:
            await self.ws_client.close()
        
        # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º
        self.analysis_queue.join()
        
        logger.info("âœ… å®æ—¶åˆ†æå¼•æ“å·²åœæ­¢")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        if hasattr(self, 'is_running') and self.is_running:
            logger.warning("âš ï¸ å®æ—¶åˆ†æå¼•æ“æœªæ­£å¸¸å…³é—­")