#!/usr/bin/env python3
"""
åˆ†æä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ä¸“ä¸šçº§å¤šå‘¨æœŸåˆ†æç»“æœæ•´åˆå’Œä¸Šä¸‹æ–‡ç®¡ç†
æä¾›é«˜çº§çš„åˆ†æç»“æœæ•´åˆã€å†å²è®°å½•ç®¡ç†å’Œå†³ç­–æ”¯æŒåŠŸèƒ½
"""

import json
import sqlite3
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
from threading import Lock

logger = logging.getLogger(__name__)

class ContextPriority(Enum):
    """ä¸Šä¸‹æ–‡ä¼˜å…ˆçº§"""
    CRITICAL = "critical"    # å…³é”®ä¿¡æ¯
    HIGH = "high"           # é«˜ä¼˜å…ˆçº§
    NORMAL = "normal"       # æ™®é€š
    LOW = "low"            # ä½ä¼˜å…ˆçº§

class SignalStrength(Enum):
    """ä¿¡å·å¼ºåº¦"""
    VERY_STRONG = "very_strong"
    STRONG = "strong"
    MODERATE = "moderate"
    WEAK = "weak"
    NEUTRAL = "neutral"
    CONFLICTING = "conflicting"

@dataclass
class ContextEvent:
    """ä¸Šä¸‹æ–‡äº‹ä»¶"""
    timestamp: datetime
    event_type: str
    description: str
    priority: ContextPriority
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp.isoformat(),
            'event_type': self.event_type,
            'description': self.description,
            'priority': self.priority.value,
            'metadata': self.metadata
        }

@dataclass
class TimeframeAnalysisContext:
    """å•æ—¶é—´æ¡†æ¶åˆ†æä¸Šä¸‹æ–‡ - Al Brooksä¸“ç”¨"""
    timeframe: str
    quality_score: float
    signal_strength: SignalStrength
    key_insights: List[str]
    risk_factors: List[str]
    volume_analysis: Dict[str, Any]
    timestamp: datetime
    
    def get_weight(self) -> float:
        """è·å–æ—¶é—´æ¡†æ¶æƒé‡"""
        timeframe_weights = {
            '1m': 0.1, '5m': 0.3, '15m': 0.5, '30m': 0.7,
            '1h': 1.0, '4h': 1.2, '1d': 1.5, '1w': 2.0
        }
        return timeframe_weights.get(self.timeframe, 1.0)

@dataclass
class MultiTimeframeContext:
    """å¤šæ—¶é—´æ¡†æ¶åˆ†æä¸Šä¸‹æ–‡"""
    symbol: str
    analysis_timestamp: datetime
    primary_timeframe: str
    timeframe_contexts: Dict[str, TimeframeAnalysisContext]
    overall_signal: SignalStrength
    consistency_score: float
    confidence_level: float
    major_confluence_zones: List[Dict[str, Any]]
    risk_warnings: List[str]
    trading_recommendations: List[str]
    
    def get_dominant_signal(self) -> Tuple[SignalStrength, float]:
        """è·å–ä¸»å¯¼ä¿¡å·å’Œç½®ä¿¡åº¦"""
        if not self.timeframe_contexts:
            return SignalStrength.NEUTRAL, 0.0
            
        weighted_signals = {}
        total_weight = 0
        
        for tf, context in self.timeframe_contexts.items():
            weight = context.get_weight()
            signal = context.signal_strength
            
            if signal not in weighted_signals:
                weighted_signals[signal] = 0
            weighted_signals[signal] += weight
            total_weight += weight
        
        if total_weight == 0:
            return SignalStrength.NEUTRAL, 0.0
            
        # è®¡ç®—æƒé‡æœ€é«˜çš„ä¿¡å·
        dominant_signal = max(weighted_signals.items(), key=lambda x: x[1])
        confidence = dominant_signal[1] / total_weight
        
        return dominant_signal[0], confidence

class AnalysisHistoryManager:
    """åˆ†æå†å²ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "logs/analysis_history.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._lock = Lock()
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS analysis_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    timeframes TEXT NOT NULL,
                    overall_signal TEXT,
                    consistency_score REAL,
                    confidence_level REAL,
                    analysis_data TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS context_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    event_type TEXT NOT NULL,
                    description TEXT,
                    priority TEXT,
                    metadata TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_symbol_timestamp ON analysis_history(symbol, timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_events_symbol ON context_events(symbol, timestamp)")
    
    def save_analysis_context(self, context: MultiTimeframeContext):
        """ä¿å­˜åˆ†æä¸Šä¸‹æ–‡"""
        with self._lock:
            try:
                with sqlite3.connect(self.db_path) as conn:
                    timeframes = list(context.timeframe_contexts.keys())
                    analysis_data = json.dumps({
                        'timeframe_contexts': {
                            tf: {
                                'timeframe': ctx.timeframe,
                                'quality_score': ctx.quality_score,
                                'signal_strength': ctx.signal_strength.value,
                                'key_insights': ctx.key_insights,
                                'risk_factors': ctx.risk_factors,
                                'volume_analysis': ctx.volume_analysis
                            }
                            for tf, ctx in context.timeframe_contexts.items()
                        },
                        'major_confluence_zones': context.major_confluence_zones,
                        'risk_warnings': context.risk_warnings,
                        'trading_recommendations': context.trading_recommendations
                    }, ensure_ascii=False)
                    
                    conn.execute("""
                        INSERT INTO analysis_history 
                        (symbol, timestamp, timeframes, overall_signal, consistency_score, confidence_level, analysis_data)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        context.symbol,
                        context.analysis_timestamp.isoformat(),
                        json.dumps(timeframes),
                        context.overall_signal.value,
                        context.consistency_score,
                        context.confidence_level,
                        analysis_data
                    ))
                    
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜åˆ†æä¸Šä¸‹æ–‡å¤±è´¥: {e}")
    
    def get_recent_analysis(self, symbol: str, hours: int = 24) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„åˆ†æè®°å½•"""
        since_time = (datetime.now() - timedelta(hours=hours)).isoformat()
        
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("""
                SELECT * FROM analysis_history 
                WHERE symbol = ? AND timestamp >= ?
                ORDER BY timestamp DESC
                LIMIT 50
            """, (symbol, since_time))
            
            return [dict(row) for row in cursor.fetchall()]
    
    def save_context_event(self, symbol: str, event: ContextEvent):
        """ä¿å­˜ä¸Šä¸‹æ–‡äº‹ä»¶"""
        with self._lock:
            try:
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute("""
                        INSERT INTO context_events 
                        (symbol, timestamp, event_type, description, priority, metadata)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        symbol,
                        event.timestamp.isoformat(),
                        event.event_type,
                        event.description,
                        event.priority.value,
                        json.dumps(event.metadata, ensure_ascii=False)
                    ))
                    
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜ä¸Šä¸‹æ–‡äº‹ä»¶å¤±è´¥: {e}")

class ConfluenceAnalyzer:
    """æ±‡èšåˆ†æå™¨ - è¯†åˆ«å¤šæ—¶é—´æ¡†æ¶çš„å…³é”®æ±‡èšåŒºåŸŸ"""
    
    def __init__(self):
        self.confluence_threshold = 0.02  # 2%ä»·æ ¼è·ç¦»å†…è§†ä¸ºæ±‡èš
    
    def find_confluence_zones(self, timeframe_contexts: Dict[str, TimeframeAnalysisContext]) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾æ±‡èšåŒºåŸŸ"""
        confluence_zones = []
        
        # Al Brooksåˆ†æä¸“æ³¨äºä»·æ ¼è¡Œä¸ºæ¨¡å¼è€Œéä¼ ç»Ÿæ”¯æ’‘é˜»åŠ›
        all_levels = []
        
        if len(all_levels) < 2:
            return confluence_zones
            
        # æŒ‰ä»·æ ¼æ’åº
        all_levels.sort(key=lambda x: x['price'])
        
        # æŸ¥æ‰¾æ±‡èšåŒºåŸŸ
        i = 0
        while i < len(all_levels):
            current_group = [all_levels[i]]
            j = i + 1
            
            # æ‰¾åˆ°ä»·æ ¼ç›¸è¿‘çš„æ‰€æœ‰æ°´å¹³
            while j < len(all_levels):
                price_diff = abs(all_levels[j]['price'] - current_group[0]['price']) / current_group[0]['price']
                if price_diff <= self.confluence_threshold:
                    current_group.append(all_levels[j])
                    j += 1
                else:
                    break
            
            # å¦‚æœæœ‰å¤šä¸ªæ—¶é—´æ¡†æ¶çš„æ±‡èšï¼Œåˆ›å»ºæ±‡èšåŒºåŸŸ
            if len(current_group) >= 2:
                confluence_zones.append(self._create_confluence_zone(current_group))
            
            i = j if j > i + 1 else i + 1
        
        # æŒ‰å¼ºåº¦æ’åº
        confluence_zones.sort(key=lambda x: x['strength'], reverse=True)
        return confluence_zones[:5]  # è¿”å›å‰5ä¸ªæœ€å¼ºçš„æ±‡èšåŒºåŸŸ
    
    def _create_confluence_zone(self, levels: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ›å»ºæ±‡èšåŒºåŸŸ"""
        avg_price = sum(level['price'] for level in levels) / len(levels)
        total_weight = sum(level['weight'] for level in levels)
        avg_quality = sum(level['quality'] for level in levels) / len(levels)
        
        timeframes_involved = list(set(level['timeframe'] for level in levels))
        level_types = list(set(level['type'] for level in levels))
        
        # è®¡ç®—å¼ºåº¦ (åŸºäºæƒé‡ã€æ—¶é—´æ¡†æ¶æ•°é‡å’Œè´¨é‡)
        strength = total_weight * len(timeframes_involved) * (avg_quality / 100)
        
        return {
            'price': avg_price,
            'strength': strength,
            'timeframes': timeframes_involved,
            'level_types': level_types,
            'total_weight': total_weight,
            'average_quality': avg_quality,
            'levels_count': len(levels)
        }

class AnalysisContext:
    """
    åˆ†æä¸Šä¸‹æ–‡ç®¡ç†å™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å¤šæ—¶é—´æ¡†æ¶åˆ†æç»“æœæ•´åˆ
    2. å†å²åˆ†æè®°å½•ç®¡ç†
    3. æ±‡èšåŒºåŸŸè¯†åˆ«å’Œåˆ†æ
    4. ä¸Šä¸‹æ–‡äº‹ä»¶è·Ÿè¸ª
    5. æ™ºèƒ½å†³ç­–æ”¯æŒ
    """
    
    def __init__(self, db_path: str = "logs/analysis_history.db"):
        self.history_manager = AnalysisHistoryManager(db_path)
        self.confluence_analyzer = ConfluenceAnalyzer()
        logger.info("âœ… åˆ†æä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_multi_timeframe_context(self,
                                     symbol: str,
                                     primary_timeframe: str,
                                     analysis_results: Dict[str, Dict[str, Any]]) -> MultiTimeframeContext:
        """
        åˆ›å»ºå¤šæ—¶é—´æ¡†æ¶åˆ†æä¸Šä¸‹æ–‡
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            primary_timeframe: ä¸»è¦æ—¶é—´æ¡†æ¶
            analysis_results: å„æ—¶é—´æ¡†æ¶çš„åˆ†æç»“æœ
            
        Returns:
            å¤šæ—¶é—´æ¡†æ¶åˆ†æä¸Šä¸‹æ–‡
        """
        timestamp = datetime.now()
        timeframe_contexts = {}
        
        # å¤„ç†å„æ—¶é—´æ¡†æ¶åˆ†æç»“æœ
        for timeframe, result in analysis_results.items():
            if not result.get('success', False):
                continue
                
            # ä»åˆ†æç»“æœä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
            context = self._extract_timeframe_context(timeframe, result, timestamp)
            timeframe_contexts[timeframe] = context
        
        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        overall_signal = self._calculate_overall_signal(timeframe_contexts)
        consistency_score = self._calculate_consistency_score(analysis_results)
        confidence_level = self._calculate_confidence_level(timeframe_contexts, consistency_score)
        
        # æŸ¥æ‰¾æ±‡èšåŒºåŸŸ
        confluence_zones = self.confluence_analyzer.find_confluence_zones(timeframe_contexts)
        
        # ç”Ÿæˆé£é™©è­¦å‘Šå’Œäº¤æ˜“å»ºè®®
        risk_warnings = self._generate_risk_warnings(timeframe_contexts, consistency_score)
        trading_recommendations = self._generate_trading_recommendations(
            timeframe_contexts, confluence_zones, overall_signal
        )
        
        context = MultiTimeframeContext(
            symbol=symbol,
            analysis_timestamp=timestamp,
            primary_timeframe=primary_timeframe,
            timeframe_contexts=timeframe_contexts,
            overall_signal=overall_signal,
            consistency_score=consistency_score,
            confidence_level=confidence_level,
            major_confluence_zones=confluence_zones,
            risk_warnings=risk_warnings,
            trading_recommendations=trading_recommendations
        )
        
        # ä¿å­˜åˆ°å†å²è®°å½•
        self.history_manager.save_analysis_context(context)
        
        logger.info(f"ğŸ“Š åˆ›å»ºå¤šæ—¶é—´æ¡†æ¶ä¸Šä¸‹æ–‡ - {symbol}, ä¸€è‡´æ€§: {consistency_score:.1f}, ä¿¡å¿ƒ: {confidence_level:.1f}")
        return context
    
    def _extract_timeframe_context(self, timeframe: str, result: Dict[str, Any], timestamp: datetime) -> TimeframeAnalysisContext:
        """ä»åˆ†æç»“æœæå–æ—¶é—´æ¡†æ¶ä¸Šä¸‹æ–‡"""
        
        # ä»AIåˆ†ææ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        analysis_text = result.get('analysis_text', result.get('analysis', ''))
        
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
        key_insights = self._extract_insights(analysis_text)
        risk_factors = self._extract_risk_factors(analysis_text)
        
        # ç”Ÿæˆä¿¡å·å¼ºåº¦
        quality_score = result.get('quality_score', 50)
        signal_strength = self._determine_signal_strength(analysis_text, quality_score)
        
        return TimeframeAnalysisContext(
            timeframe=timeframe,
            quality_score=quality_score,
            signal_strength=signal_strength,
            key_insights=key_insights,
            risk_factors=risk_factors,
            volume_analysis={'pattern': 'normal'},  # ç®€åŒ–
            timestamp=timestamp
        )
    
    def _extract_insights(self, text: str) -> List[str]:
        """æå–å…³é”®æ´å¯Ÿ"""
        insights = []
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        insight_keywords = [
            ('çªç ´', 'ä»·æ ¼çªç ´å…³é”®æ°´å¹³'),
            ('æ”¯æ’‘', 'å‘ç°é‡è¦æ”¯æ’‘ä½'),
            ('é˜»åŠ›', 'è¯†åˆ«å…³é”®é˜»åŠ›ä½'),
            ('æˆäº¤é‡', 'æˆäº¤é‡æ¨¡å¼åˆ†æ'),
            ('è¶‹åŠ¿', 'è¶‹åŠ¿æ–¹å‘åˆ†æ')
        ]
        
        for keyword, insight in insight_keywords:
            if keyword in text:
                insights.append(insight)
        
        return insights[:3]  # æœ€å¤š3ä¸ªå…³é”®æ´å¯Ÿ
    
    def _extract_risk_factors(self, text: str) -> List[str]:
        """æå–é£é™©å› ç´ """
        risks = []
        
        risk_keywords = [
            ('æ³¢åŠ¨', 'é«˜æ³¢åŠ¨æ€§é£é™©'),
            ('æµåŠ¨æ€§', 'æµåŠ¨æ€§é£é™©'),
            ('èƒŒç¦»', 'æŠ€æœ¯æŒ‡æ ‡èƒŒç¦»é£é™©'),
            ('ä¸ç¡®å®š', 'å¸‚åœºä¸ç¡®å®šæ€§é£é™©')
        ]
        
        for keyword, risk in risk_keywords:
            if keyword in text:
                risks.append(risk)
        
        return risks[:2]  # æœ€å¤š2ä¸ªé£é™©å› ç´ 
    
    
    def _determine_signal_strength(self, text: str, quality_score: float) -> SignalStrength:
        """ç¡®å®šä¿¡å·å¼ºåº¦"""
        if quality_score < 40:
            return SignalStrength.WEAK
        elif quality_score < 60:
            return SignalStrength.MODERATE
        elif quality_score < 80:
            return SignalStrength.STRONG
        else:
            return SignalStrength.VERY_STRONG
    
    def _calculate_overall_signal(self, contexts: Dict[str, TimeframeAnalysisContext]) -> SignalStrength:
        """è®¡ç®—æ•´ä½“ä¿¡å·å¼ºåº¦"""
        if not contexts:
            return SignalStrength.NEUTRAL
            
        # åŸºäºè´¨é‡è¯„åˆ†åŠ æƒå¹³å‡
        weighted_sum = 0
        total_weight = 0
        
        for context in contexts.values():
            weight = context.get_weight()
            signal_value = self._signal_to_value(context.signal_strength)
            weighted_sum += signal_value * weight
            total_weight += weight
        
        if total_weight == 0:
            return SignalStrength.NEUTRAL
            
        avg_signal = weighted_sum / total_weight
        return self._value_to_signal(avg_signal)
    
    def _signal_to_value(self, signal: SignalStrength) -> float:
        """ä¿¡å·å¼ºåº¦è½¬æ•°å€¼"""
        mapping = {
            SignalStrength.VERY_STRONG: 5.0,
            SignalStrength.STRONG: 4.0,
            SignalStrength.MODERATE: 3.0,
            SignalStrength.WEAK: 2.0,
            SignalStrength.NEUTRAL: 1.0,
            SignalStrength.CONFLICTING: 0.5
        }
        return mapping.get(signal, 1.0)
    
    def _value_to_signal(self, value: float) -> SignalStrength:
        """æ•°å€¼è½¬ä¿¡å·å¼ºåº¦"""
        if value >= 4.5:
            return SignalStrength.VERY_STRONG
        elif value >= 3.5:
            return SignalStrength.STRONG
        elif value >= 2.5:
            return SignalStrength.MODERATE
        elif value >= 1.5:
            return SignalStrength.WEAK
        else:
            return SignalStrength.NEUTRAL
    
    def _calculate_consistency_score(self, results: Dict[str, Dict[str, Any]]) -> float:
        """è®¡ç®—ä¸€è‡´æ€§è¯„åˆ†"""
        successful_results = [r for r in results.values() if r.get('success', False)]
        
        if len(successful_results) < 2:
            return 100.0 if successful_results else 0.0
            
        quality_scores = [r.get('quality_score', 50) for r in successful_results]
        
        # åŸºäºè´¨é‡è¯„åˆ†æ ‡å‡†å·®è®¡ç®—ä¸€è‡´æ€§
        import statistics
        if len(quality_scores) > 1:
            std_dev = statistics.stdev(quality_scores)
            consistency = max(0, 100 - (std_dev * 1.5))
        else:
            consistency = 100.0
            
        return min(100.0, consistency)
    
    def _calculate_confidence_level(self, contexts: Dict[str, TimeframeAnalysisContext], consistency_score: float) -> float:
        """è®¡ç®—ä¿¡å¿ƒæ°´å¹³"""
        if not contexts:
            return 0.0
            
        avg_quality = sum(ctx.quality_score for ctx in contexts.values()) / len(contexts)
        timeframe_count = len(contexts)
        
        # ä¿¡å¿ƒæ°´å¹³ = (å¹³å‡è´¨é‡ * 0.4) + (ä¸€è‡´æ€§ * 0.4) + (æ—¶é—´æ¡†æ¶æ•°é‡å¥–åŠ± * 0.2)
        timeframe_bonus = min(20, timeframe_count * 5)  # æ¯ä¸ªæ—¶é—´æ¡†æ¶+5åˆ†ï¼Œæœ€å¤š20åˆ†
        confidence = (avg_quality * 0.4) + (consistency_score * 0.4) + timeframe_bonus
        
        return min(100.0, confidence)
    
    def _generate_risk_warnings(self, contexts: Dict[str, TimeframeAnalysisContext], consistency_score: float) -> List[str]:
        """ç”Ÿæˆé£é™©è­¦å‘Š"""
        warnings = []
        
        if consistency_score < 50:
            warnings.append("å¤šæ—¶é—´æ¡†æ¶ä¿¡å·ä¸¥é‡å†²çªï¼Œå»ºè®®ç­‰å¾…ç¡®è®¤")
            
        low_quality_count = sum(1 for ctx in contexts.values() if ctx.quality_score < 60)
        if low_quality_count > 0:
            warnings.append(f"{low_quality_count}ä¸ªæ—¶é—´æ¡†æ¶åˆ†æè´¨é‡è¾ƒä½")
            
        # æ£€æŸ¥æ˜¯å¦æœ‰é£é™©å› ç´ 
        all_risks = []
        for ctx in contexts.values():
            all_risks.extend(ctx.risk_factors)
        
        if all_risks:
            warnings.append("è¯†åˆ«åˆ°æ½œåœ¨é£é™©å› ç´ ï¼Œè¯·è°¨æ…äº¤æ˜“")
            
        return warnings
    
    def _generate_trading_recommendations(self,
                                        contexts: Dict[str, TimeframeAnalysisContext],
                                        confluence_zones: List[Dict[str, Any]],
                                        overall_signal: SignalStrength) -> List[str]:
        """ç”Ÿæˆäº¤æ˜“å»ºè®®"""
        recommendations = []
        
        if overall_signal == SignalStrength.VERY_STRONG:
            recommendations.append("å¼ºçƒˆä¿¡å·ç¡®è®¤ï¼Œå¯è€ƒè™‘åŠ å¤§ä»“ä½")
        elif overall_signal == SignalStrength.STRONG:
            recommendations.append("ä¿¡å·è¾ƒå¼ºï¼Œå»ºè®®æ ‡å‡†ä»“ä½äº¤æ˜“")
        elif overall_signal == SignalStrength.MODERATE:
            recommendations.append("ä¿¡å·ä¸€èˆ¬ï¼Œå»ºè®®å°ä»“ä½è¯•æ¢")
        else:
            recommendations.append("ä¿¡å·è¾ƒå¼±ï¼Œå»ºè®®è§‚æœ›æˆ–é™ä½ä»“ä½")
            
        if confluence_zones:
            recommendations.append(f"å‘ç°{len(confluence_zones)}ä¸ªå…³é”®æ±‡èšåŒºåŸŸï¼Œå¯ä½œä¸ºé‡è¦å‚è€ƒæ°´å¹³")
            
        return recommendations
    
    def get_analysis_history(self, symbol: str, hours: int = 24) -> List[Dict[str, Any]]:
        """è·å–åˆ†æå†å²"""
        return self.history_manager.get_recent_analysis(symbol, hours)
    
    def add_context_event(self, symbol: str, event_type: str, description: str, 
                         priority: ContextPriority = ContextPriority.NORMAL,
                         metadata: Dict[str, Any] = None):
        """æ·»åŠ ä¸Šä¸‹æ–‡äº‹ä»¶"""
        event = ContextEvent(
            timestamp=datetime.now(),
            event_type=event_type,
            description=description,
            priority=priority,
            metadata=metadata or {}
        )
        
        self.history_manager.save_context_event(symbol, event)
        logger.info(f"ğŸ“ æ·»åŠ ä¸Šä¸‹æ–‡äº‹ä»¶ - {symbol}: {description}")