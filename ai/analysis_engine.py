import pandas as pd
from typing import Dict, List, Any, Optional
import logging
from .openrouter_client import OpenRouterClient
from .multi_model_validator import MultiModelValidator, ValidationConfig
from data import DataProcessor

logger = logging.getLogger(__name__)

class AnalysisEngine:
    """
    å¢å¼ºåˆ†æå¼•æ“ - é›†æˆå¤šæ¨¡å‹éªŒè¯æœºåˆ¶
    
    æ–°åŠŸèƒ½ï¼š
    1. å¤šæ¨¡å‹äº¤å‰éªŒè¯é˜²å¹»è§‰
    2. å…±è¯†å¾—åˆ†è®¡ç®—
    3. åˆ†æ­§æ£€æµ‹ä¸æŠ¥è­¦
    4. æ™ºèƒ½ä»²è£æœºåˆ¶
    5. ç½®ä¿¡åº¦è¯„ä¼°
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    - validated_vpa_analysis(): å®Œæ•´éªŒè¯åˆ†æï¼ˆæ¨èï¼‰
    - quick_validation_check(): å¿«é€ŸéªŒè¯æ£€æŸ¥
    - comprehensive_analysis(): åŸæœ‰çš„ç»¼åˆåˆ†æï¼ˆå‘åå…¼å®¹ï¼‰
    """
    
    def __init__(self, api_key: Optional[str] = None, enable_validation: bool = True):
        self.client = OpenRouterClient(api_key)
        self.processor = DataProcessor()
        
        # åˆå§‹åŒ–å¤šæ¨¡å‹éªŒè¯å™¨
        if enable_validation:
            self.validator = MultiModelValidator(api_key)
            logger.info("âœ… å¤šæ¨¡å‹éªŒè¯å·²å¯ç”¨")
        else:
            self.validator = None
            logger.info("â„¹ï¸ å¤šæ¨¡å‹éªŒè¯å·²ç¦ç”¨")
    
    def comprehensive_analysis(self, 
                             df: pd.DataFrame,
                             models: List[str] = ['gpt4'],
                             include_indicators: bool = True,
                             include_patterns: bool = True) -> Dict[str, Any]:
        """
        ç»¼åˆåˆ†æï¼šä½¿ç”¨å¤šä¸ªæ¨¡å‹åˆ†æåŒä¸€æ•°æ®
        """
        try:
            # æ•°æ®é¢„å¤„ç†
            if include_indicators:
                df = self.processor.add_basic_indicators(df)
            
            if include_patterns:
                df = self.processor.detect_candlestick_patterns(df)
                df = self.processor.analyze_volume_price_relationship(df)
            
            # è·å–å…³é”®æ°´å¹³
            key_levels = self.processor.get_key_levels(df)
            
            # åˆ›å»ºåˆ†ææŠ¥å‘Š
            report = {
                'data_summary': {
                    'total_bars': len(df),
                    'time_range': {
                        'start': df['datetime'].iloc[0],
                        'end': df['datetime'].iloc[-1]
                    },
                    'price_range': {
                        'min': df['low'].min(),
                        'max': df['high'].max(),
                        'current': df['close'].iloc[-1],
                        'change': ((df['close'].iloc[-1] / df['close'].iloc[0]) - 1) * 100
                    },
                    'volume_summary': {
                        'total': df['volume'].sum(),
                        'average': df['volume'].mean(),
                        'recent_avg': df['volume'].tail(10).mean()
                    }
                },
                'key_levels': key_levels,
                'ai_analysis': {}
            }
            
            # ä½¿ç”¨ä¸åŒæ¨¡å‹è¿›è¡Œåˆ†æ
            for model in models:
                logger.info(f"ä½¿ç”¨ {model} æ¨¡å‹è¿›è¡Œåˆ†æ...")
                
                # VPAåˆ†æ
                vpa_data = self._prepare_vpa_data(df)
                vpa_result = self.client.analyze_market_data(
                    data=vpa_data,
                    model_name=model,
                    analysis_type='vpa'
                )
                
                # æŠ€æœ¯åˆ†æ
                technical_data = self._prepare_technical_data(df, key_levels)
                technical_result = self.client.analyze_market_data(
                    data=technical_data,
                    model_name=model,
                    analysis_type='technical'
                )
                
                # å½¢æ€åˆ†æ
                pattern_data = self._prepare_pattern_data(df)
                pattern_result = self.client.analyze_market_data(
                    data=pattern_data,
                    model_name=model,
                    analysis_type='pattern'
                )
                
                report['ai_analysis'][model] = {
                    'vpa_analysis': vpa_result,
                    'technical_analysis': technical_result,
                    'pattern_analysis': pattern_result,
                    'total_tokens': (
                        vpa_result.get('usage', {}).get('total_tokens', 0) +
                        technical_result.get('usage', {}).get('total_tokens', 0) +
                        pattern_result.get('usage', {}).get('total_tokens', 0)
                    )
                }
            
            return report
            
        except Exception as e:
            logger.error(f"ç»¼åˆåˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _prepare_vpa_data(self, df: pd.DataFrame, last_n: int = 50) -> str:
        """
        ä¸ºVPAåˆ†æå‡†å¤‡æ•°æ®æ ¼å¼
        """
        recent_df = df.tail(last_n).copy()
        
        # åŸºç¡€OHLCVæ•°æ®
        data_lines = ["# ETH/USDT 1å°æ—¶Kçº¿æ•°æ® (VPAåˆ†æ)\\n"]
        data_lines.append("æ—¶é—´,å¼€ç›˜ä»·,æœ€é«˜ä»·,æœ€ä½ä»·,æ”¶ç›˜ä»·,æˆäº¤é‡,æˆäº¤é‡æ¯”ç‡,ä»·æ ¼å˜åŒ–")
        
        for _, row in recent_df.iterrows():
            volume_ratio = row.get('volume_ratio', 1.0) if 'volume_ratio' in row else 1.0
            price_change = row.get('price_change', 0) * 100 if 'price_change' in row else 0
            
            line = f"{row['datetime']},{row['open']:.2f},{row['high']:.2f},{row['low']:.2f},{row['close']:.2f},{row['volume']:.0f},{volume_ratio:.2f},{price_change:+.2f}%"
            data_lines.append(line)
        
        # æ·»åŠ VPAä¿¡å·æ€»ç»“
        if 'bullish_volume' in recent_df.columns:
            bullish_signals = recent_df['bullish_volume'].sum()
            bearish_signals = recent_df['bearish_volume'].sum()
            suspicious_rallies = recent_df['suspicious_rally'].sum()
            suspicious_declines = recent_df['suspicious_decline'].sum()
            
            data_lines.append(f"\\n# VPAä¿¡å·ç»Ÿè®¡ (æœ€è¿‘{last_n}æ ¹Kçº¿)")
            data_lines.append(f"å¥åº·ä¸Šæ¶¨ä¿¡å·: {bullish_signals}")
            data_lines.append(f"å¥åº·ä¸‹è·Œä¿¡å·: {bearish_signals}")
            data_lines.append(f"å¯ç–‘ä¸Šæ¶¨ä¿¡å·: {suspicious_rallies}")
            data_lines.append(f"å¯ç–‘ä¸‹è·Œä¿¡å·: {suspicious_declines}")
        
        return "\\n".join(data_lines)
    
    def _prepare_technical_data(self, df: pd.DataFrame, key_levels: Dict, last_n: int = 50) -> str:
        """
        ä¸ºæŠ€æœ¯åˆ†æå‡†å¤‡æ•°æ®æ ¼å¼
        """
        recent_df = df.tail(last_n).copy()
        
        data_lines = ["# ETH/USDT æŠ€æœ¯åˆ†ææ•°æ®\\n"]
        
        # å…³é”®æ°´å¹³
        data_lines.append("## å…³é”®æŠ€æœ¯æ°´å¹³")
        data_lines.append(f"é˜»åŠ›ä½: {key_levels.get('resistance_levels', [])}")
        data_lines.append(f"æ”¯æ’‘ä½: {key_levels.get('support_levels', [])}")
        data_lines.append("")
        
        # å¸¦æŠ€æœ¯æŒ‡æ ‡çš„OHLCVæ•°æ®
        data_lines.append("## Kçº¿æ•°æ®ä¸æŠ€æœ¯æŒ‡æ ‡")
        header = "æ—¶é—´,å¼€ç›˜,æœ€é«˜,æœ€ä½,æ”¶ç›˜,æˆäº¤é‡"
        
        # æ·»åŠ å¯ç”¨çš„æŠ€æœ¯æŒ‡æ ‡åˆ—
        indicators = ['sma_20', 'rsi', 'macd', 'bb_upper', 'bb_lower', 'vwap']
        available_indicators = [ind for ind in indicators if ind in recent_df.columns]
        if available_indicators:
            header += "," + ",".join(available_indicators)
        
        data_lines.append(header)
        
        for _, row in recent_df.iterrows():
            line = f"{row['datetime']},{row['open']:.2f},{row['high']:.2f},{row['low']:.2f},{row['close']:.2f},{row['volume']:.0f}"
            
            for indicator in available_indicators:
                value = row.get(indicator, 0)
                if pd.isna(value):
                    value = 0
                line += f",{value:.2f}"
            
            data_lines.append(line)
        
        return "\\n".join(data_lines)
    
    def _prepare_pattern_data(self, df: pd.DataFrame, last_n: int = 30) -> str:
        """
        ä¸ºå½¢æ€åˆ†æå‡†å¤‡æ•°æ®æ ¼å¼
        """
        recent_df = df.tail(last_n).copy()
        
        data_lines = ["# ETH/USDT Kçº¿å½¢æ€æ•°æ®\\n"]
        data_lines.append("æ—¶é—´,å¼€ç›˜ä»·,æœ€é«˜ä»·,æœ€ä½ä»·,æ”¶ç›˜ä»·,æˆäº¤é‡,Kçº¿æè¿°")
        
        for _, row in recent_df.iterrows():
            # åŸºç¡€Kçº¿æè¿°
            body_color = "é˜³çº¿" if row['close'] > row['open'] else "é˜´çº¿" if row['close'] < row['open'] else "åå­—"
            body_size = abs(row['close'] - row['open'])
            total_range = row['high'] - row['low']
            
            description_parts = [body_color]
            
            # æ·»åŠ å½¢æ€ä¿¡æ¯
            if 'is_doji' in row and row['is_doji']:
                description_parts.append("åå­—æ˜Ÿ")
            if 'is_hammer' in row and row['is_hammer']:
                description_parts.append("é”¤å­çº¿")
            if 'is_shooting_star' in row and row['is_shooting_star']:
                description_parts.append("æµæ˜Ÿçº¿")
            if 'bullish_engulfing' in row and row['bullish_engulfing']:
                description_parts.append("çœ‹æ¶¨åæ²¡")
            if 'bearish_engulfing' in row and row['bearish_engulfing']:
                description_parts.append("çœ‹è·Œåæ²¡")
            
            # å®ä½“å¤§å°æè¿°
            if total_range > 0:
                body_ratio = body_size / total_range
                if body_ratio < 0.1:
                    description_parts.append("å°å®ä½“")
                elif body_ratio > 0.7:
                    description_parts.append("å¤§å®ä½“")
            
            description = ",".join(description_parts)
            
            line = f"{row['datetime']},{row['open']:.2f},{row['high']:.2f},{row['low']:.2f},{row['close']:.2f},{row['volume']:.0f},{description}"
            data_lines.append(line)
        
        return "\\n".join(data_lines)
    
    def validated_vpa_analysis(self, 
                              df: pd.DataFrame,
                              enable_fast_mode: bool = False,
                              validation_config: Optional[ValidationConfig] = None) -> Dict[str, Any]:
        """
        å¸¦éªŒè¯çš„VPAåˆ†æ - å¤šæ¨¡å‹äº¤å‰éªŒè¯é˜²å¹»è§‰
        
        Args:
            df: Kçº¿æ•°æ®
            enable_fast_mode: å¿«é€Ÿæ¨¡å¼ï¼ˆåªç”¨ä¸»è¦æ¨¡å‹ï¼‰
            validation_config: éªŒè¯é…ç½®
            
        Returns:
            åŒ…å«éªŒè¯ç»“æœçš„å®Œæ•´åˆ†ææŠ¥å‘Š
        """
        try:
            if not self.validator:
                logger.warning("âš ï¸ éªŒè¯å™¨æœªå¯ç”¨ï¼Œå›é€€åˆ°å•æ¨¡å‹åˆ†æ")
                return self._fallback_single_analysis(df)
            
            logger.info("ğŸ” å¼€å§‹å¸¦éªŒè¯çš„VPAåˆ†æ...")
            
            # ä½¿ç”¨æœ€ä¼˜çš„Patternæ ¼å¼å‡†å¤‡æ•°æ®
            from formatters import DataFormatter
            formatter = DataFormatter()
            
            # åŸºäºé˜¶æ®µ2æµ‹è¯•ç»“æœï¼ŒPatternæ ¼å¼æœ€ä¼˜
            formatted_data = formatter.to_pattern_description(df)
            
            # æ‰§è¡Œå¤šæ¨¡å‹éªŒè¯åˆ†æ
            validation_result = self.validator.validate_analysis(
                data=formatted_data,
                analysis_type='vpa',
                enable_fast_mode=enable_fast_mode
            )
            
            # å‡†å¤‡æ•°æ®æ‘˜è¦
            data_summary = {
                'total_bars': len(df),
                'time_range': {
                    'start': df['datetime'].iloc[0],
                    'end': df['datetime'].iloc[-1]
                },
                'price_range': {
                    'min': df['low'].min(),
                    'max': df['high'].max(),
                    'current': df['close'].iloc[-1],
                    'change': ((df['close'].iloc[-1] / df['close'].iloc[0]) - 1) * 100
                },
                'volume_summary': {
                    'total': df['volume'].sum(),
                    'average': df['volume'].mean(),
                    'recent_avg': df['volume'].tail(10).mean()
                }
            }
            
            # æ„å»ºéªŒè¯åˆ†ææŠ¥å‘Š
            validated_report = {
                'analysis_type': 'validated_vpa',
                'data_summary': data_summary,
                'validation_summary': self.validator.get_validation_summary(validation_result),
                'consensus_analysis': self._extract_consensus_analysis(validation_result),
                'model_analyses': {
                    'primary': validation_result.primary_analysis,
                    'validation': validation_result.validation_analyses
                },
                'quality_indicators': {
                    'consensus_score': validation_result.consensus_score,
                    'confidence_level': validation_result.confidence_level,
                    'model_agreement': len(validation_result.disagreements) == 0,
                    'has_arbitration': validation_result.arbitration_result is not None
                },
                'risk_assessment': self._generate_risk_assessment(validation_result),
                'performance_metrics': {
                    'total_cost': validation_result.total_cost,
                    'processing_time': validation_result.processing_time,
                    'models_used': len(validation_result.primary_analysis) + len(validation_result.validation_analyses)
                }
            }
            
            # å¦‚æœå­˜åœ¨åˆ†æ­§ï¼Œæ·»åŠ è¯¦ç»†åˆ†æ­§æŠ¥å‘Š
            if validation_result.disagreements:
                validated_report['disagreement_analysis'] = {
                    'disagreements': validation_result.disagreements,
                    'arbitration_available': validation_result.arbitration_result is not None,
                    'recommendation': self._get_disagreement_recommendation(validation_result)
                }
            
            logger.info(f"âœ… éªŒè¯åˆ†æå®Œæˆ - å…±è¯†å¾—åˆ†: {validation_result.consensus_score:.2f}")
            return validated_report
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯åˆ†æå¤±è´¥: {e}")
            return {
                'error': f"éªŒè¯åˆ†æå¤±è´¥: {str(e)}",
                'fallback_analysis': self._fallback_single_analysis(df)
            }
    
    def _fallback_single_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åå¤‡å•æ¨¡å‹åˆ†æ"""
        logger.info("ğŸ”„ æ‰§è¡Œåå¤‡å•æ¨¡å‹åˆ†æ")
        
        try:
            from formatters import DataFormatter
            formatter = DataFormatter()
            data = formatter.to_pattern_description(df)
            
            # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œåˆ†æ
            result = self.client.analyze_market_data(
                data=data,
                model_name='gpt5-mini',  # åŸºäºPhase2æµ‹è¯•çš„æœ€ä½³æ¨¡å‹
                analysis_type='vpa'
            )
            
            return {
                'analysis_type': 'single_model_fallback',
                'model_used': 'gpt5-mini',
                'analysis': result,
                'warning': 'ç”±äºéªŒè¯å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨å•æ¨¡å‹åˆ†æï¼Œè¯·æ³¨æ„æ½œåœ¨çš„å¹»è§‰é£é™©'
            }
            
        except Exception as e:
            logger.error(f"âŒ åå¤‡åˆ†æä¹Ÿå¤±è´¥: {e}")
            return {'error': f"æ‰€æœ‰åˆ†ææ–¹æ³•éƒ½å¤±è´¥: {str(e)}"}
    
    def _extract_consensus_analysis(self, validation_result) -> Dict[str, Any]:
        """ä»éªŒè¯ç»“æœä¸­æå–å…±è¯†åˆ†æ"""
        if not validation_result.primary_analysis:
            return {}
        
        # ä½¿ç”¨å…±è¯†è®¡ç®—å™¨ç”Ÿæˆæ‘˜è¦
        all_results = {**validation_result.primary_analysis, **validation_result.validation_analyses}
        
        if hasattr(self.validator, 'consensus_calc'):
            consensus_summary = self.validator.consensus_calc.generate_consensus_summary(
                all_results, validation_result.consensus_score
            )
            return consensus_summary
        
        return {'consensus_score': validation_result.consensus_score}
    
    def _generate_risk_assessment(self, validation_result) -> Dict[str, Any]:
        """ç”Ÿæˆé£é™©è¯„ä¼°"""
        risk_level = 'low'
        risk_factors = []
        
        # åŸºäºå…±è¯†å¾—åˆ†è¯„ä¼°é£é™©
        if validation_result.consensus_score < 0.4:
            risk_level = 'high'
            risk_factors.append('æ¨¡å‹é—´å­˜åœ¨ä¸¥é‡åˆ†æ­§')
        elif validation_result.consensus_score < 0.6:
            risk_level = 'medium'
            risk_factors.append('æ¨¡å‹é—´å­˜åœ¨ä¸€å®šåˆ†æ­§')
        
        # æ£€æŸ¥å…¶ä»–é£é™©å› ç´ 
        if len(validation_result.primary_analysis) < 2:
            risk_factors.append('ä¸»è¦åˆ†ææ¨¡å‹æ•°é‡ä¸è¶³')
        
        if validation_result.disagreements:
            risk_factors.append(f'å‘ç°{len(validation_result.disagreements)}ä¸ªå…·ä½“åˆ†æ­§ç‚¹')
        
        return {
            'risk_level': risk_level,
            'risk_factors': risk_factors,
            'confidence_score': validation_result.consensus_score,
            'reliability_rating': validation_result.confidence_level
        }
    
    def _get_disagreement_recommendation(self, validation_result) -> str:
        """è·å–åˆ†æ­§å¤„ç†å»ºè®®"""
        if validation_result.consensus_score < 0.4:
            return "å¼ºçƒˆå»ºè®®äººå·¥å¤æ ¸ï¼Œæ¨¡å‹é—´å­˜åœ¨é‡å¤§åˆ†æ­§ï¼Œç›´æ¥ä½¿ç”¨ç»“æœé£é™©å¾ˆé«˜"
        elif validation_result.consensus_score < 0.6:
            return "å»ºè®®è°¨æ…ä½¿ç”¨ï¼Œç»“åˆäººå·¥åˆ¤æ–­å’Œé¢å¤–ä¿¡æ¯æºè¿›è¡Œå†³ç­–"
        else:
            return "åˆ†æ­§è¾ƒå°ï¼Œå¯ä»¥å‚è€ƒä¸»æµè§‚ç‚¹ï¼Œä½†å»ºè®®å…³æ³¨å°‘æ•°æ´¾æ„è§"
    
    def quick_validation_check(self, data: str, analysis_type: str = 'vpa') -> Dict[str, Any]:
        """
        å¿«é€ŸéªŒè¯æ£€æŸ¥ - åªç”¨2ä¸ªä¸»è¦æ¨¡å‹è¿›è¡Œå¿«é€Ÿäº¤å‰éªŒè¯
        
        é€‚ç”¨åœºæ™¯ï¼š
        - å®æ—¶äº¤æ˜“å†³ç­–
        - æˆæœ¬æ•æ„Ÿçš„åˆ†æ
        - å¿«é€ŸéªŒè¯éœ€æ±‚
        """
        if not self.validator:
            return {'error': 'éªŒè¯å™¨æœªå¯ç”¨'}
        
        try:
            logger.info("âš¡ æ‰§è¡Œå¿«é€ŸéªŒè¯æ£€æŸ¥...")
            
            # ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼Œåªè°ƒç”¨ä¸»è¦æ¨¡å‹
            validation_result = self.validator.validate_analysis(
                data=data,
                analysis_type=analysis_type,
                enable_fast_mode=True
            )
            
            return {
                'quick_validation': True,
                'consensus_score': validation_result.consensus_score,
                'confidence_level': validation_result.confidence_level,
                'has_disagreements': len(validation_result.disagreements) > 0,
                'processing_time': validation_result.processing_time,
                'cost': validation_result.total_cost,
                'recommendation': self.validator._get_usage_recommendation(validation_result)
            }
            
        except Exception as e:
            logger.error(f"âŒ å¿«é€ŸéªŒè¯å¤±è´¥: {e}")
            return {'error': f'å¿«é€ŸéªŒè¯å¤±è´¥: {str(e)}'}