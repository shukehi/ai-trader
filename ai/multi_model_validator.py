#!/usr/bin/env python3
"""
å¤šæ¨¡å‹éªŒè¯å¼•æ“
é˜²æ­¢å•ä¸€æ¨¡å‹å¹»è§‰ï¼Œæä¾›äº¤å‰éªŒè¯å’Œå…±è¯†æœºåˆ¶
"""

import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

from .openrouter_client import OpenRouterClient
from .consensus_calculator import ConsensusCalculator

logger = logging.getLogger(__name__)

@dataclass
class ValidationConfig:
    """éªŒè¯é…ç½® (ä¼˜åŒ–ç‰ˆ - å¯¹æŠ—æ¨¡å‹åå·®)"""
    primary_models: Optional[List[str]] = None
    validation_models: Optional[List[str]] = None  
    consensus_threshold: float = 0.7      # æé«˜é˜ˆå€¼ç¡®ä¿è´¨é‡
    enable_arbitration: bool = True       # å¯ç”¨ä»²è£å¤„ç†åˆ†æ­§
    max_models: int = 3                   # ä¸“æ³¨äºé«˜è´¨é‡æ¨¡å‹
    timeout_seconds: int = 120
    # åå·®æ£€æµ‹è®¾ç½®
    direction_bias_threshold: float = 0.8  # æ£€æµ‹æ–¹å‘åå·®çš„é˜ˆå€¼
    enable_bias_correction: bool = True    # å¯ç”¨åå·®çº æ­£
    
    def __post_init__(self):
        if self.primary_models is None:
            # å®ç”¨çš„æ¨¡å‹ç»„åˆ - å¯¹æŠ—æ–¹å‘æ€§åå·®
            self.primary_models = ['gpt4o-mini', 'claude-haiku']
        if self.validation_models is None:
            # å¹³è¡¡éªŒè¯æ¨¡å‹ï¼Œæä¾›ä¸åŒè§†è§’
            self.validation_models = ['gemini-flash']

@dataclass 
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    consensus_score: float
    confidence_level: str  # 'high', 'medium', 'low'
    primary_analysis: Dict[str, Any]
    validation_analyses: Dict[str, Any]
    disagreements: List[str]
    arbitration_result: Optional[Dict[str, Any]] = None
    total_cost: float = 0.0
    processing_time: float = 0.0

class MultiModelValidator:
    """
    å¤šæ¨¡å‹éªŒè¯å¼•æ“
    
    åŠŸèƒ½ï¼š
    1. å¹¶è¡Œè°ƒç”¨å¤šä¸ªLLMæ¨¡å‹
    2. äº¤å‰éªŒè¯åˆ†æç»“æœ
    3. è®¡ç®—å…±è¯†å¾—åˆ†
    4. æ£€æµ‹åˆ†æ­§å¹¶æŠ¥è­¦
    5. æ™ºèƒ½ä»²è£æœºåˆ¶
    """
    
    def __init__(self, api_key: Optional[str] = None, config: Optional[ValidationConfig] = None):
        self.client = OpenRouterClient(api_key)
        self.config = config or ValidationConfig()
        self.consensus_calc = ConsensusCalculator()
        
        logger.info(f"âœ… MultiModelValidatoråˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ä¸»è¦æ¨¡å‹: {self.config.primary_models}")
        logger.info(f"éªŒè¯æ¨¡å‹: {self.config.validation_models}")
        logger.info(f"å…±è¯†é˜ˆå€¼: {self.config.consensus_threshold}")
    
    def validate_analysis(self, 
                         data: str, 
                         analysis_type: str = 'vpa',
                         enable_fast_mode: bool = False) -> ValidationResult:
        """
        æ‰§è¡Œå¤šæ¨¡å‹éªŒè¯åˆ†æ
        
        Args:
            data: æ ¼å¼åŒ–çš„å¸‚åœºæ•°æ®
            analysis_type: åˆ†æç±»å‹
            enable_fast_mode: æ˜¯å¦å¯ç”¨å¿«é€Ÿæ¨¡å¼ï¼ˆåªç”¨primary modelsï¼‰
            
        Returns:
            ValidationResult: éªŒè¯ç»“æœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹å¤šæ¨¡å‹éªŒè¯åˆ†æ (ç±»å‹: {analysis_type})")
            
            # ç¡®å®šè¦ä½¿ç”¨çš„æ¨¡å‹
            models_to_use = self._select_models(enable_fast_mode)
            logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {models_to_use}")
            
            # å¹¶è¡Œè°ƒç”¨å¤šä¸ªæ¨¡å‹
            model_results = self._parallel_analyze(data, models_to_use, analysis_type)
            
            # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
            successful_results = {k: v for k, v in model_results.items() 
                                if v.get('success', False)}
            
            if len(successful_results) < 2:
                logger.warning(f"âš ï¸ åªæœ‰{len(successful_results)}ä¸ªæ¨¡å‹æˆåŠŸï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆéªŒè¯")
                return self._create_fallback_result(model_results, start_time)
            
            # è®¡ç®—å…±è¯†å¾—åˆ†
            consensus_score = self.consensus_calc.calculate_consensus(successful_results)
            
            # ç¡®å®šç½®ä¿¡åº¦ç­‰çº§
            confidence_level = self._determine_confidence_level(consensus_score)
            
            # æ£€æµ‹åˆ†æ­§
            disagreements = self._detect_disagreements(successful_results, consensus_score)
            
            # å‡†å¤‡ä¸»è¦åˆ†æå’ŒéªŒè¯åˆ†æ
            primary_results = {k: v for k, v in successful_results.items() 
                             if self.config.primary_models and k in self.config.primary_models}
            validation_results = {k: v for k, v in successful_results.items() 
                                if self.config.validation_models and k in self.config.validation_models}
            
            # æ‰§è¡Œä»²è£ï¼ˆå¦‚æœéœ€è¦ï¼‰
            arbitration_result = None
            if (consensus_score < self.config.consensus_threshold and 
                self.config.enable_arbitration and 
                len(disagreements) > 0):
                arbitration_result = self._execute_arbitration(data, analysis_type, disagreements)
            
            # è®¡ç®—æ€»æˆæœ¬
            total_cost = sum(r.get('cost_estimate', {}).get('estimated_cost', 0) 
                           for r in successful_results.values())
            
            processing_time = time.time() - start_time
            
            result = ValidationResult(
                consensus_score=consensus_score,
                confidence_level=confidence_level,
                primary_analysis=primary_results,
                validation_analyses=validation_results,
                disagreements=disagreements,
                arbitration_result=arbitration_result,
                total_cost=total_cost,
                processing_time=processing_time
            )
            
            self._log_validation_summary(result)
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¤šæ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            processing_time = time.time() - start_time
            return ValidationResult(
                consensus_score=0.0,
                confidence_level='low',
                primary_analysis={},
                validation_analyses={},
                disagreements=[f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}"],
                processing_time=processing_time
            )
    
    def _select_models(self, enable_fast_mode: bool) -> List[str]:
        """é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹"""
        if enable_fast_mode:
            return (self.config.primary_models or [])[:2]  # åªç”¨å‰2ä¸ªä¸»è¦æ¨¡å‹
        
        primary_models = self.config.primary_models or []
        validation_models = self.config.validation_models or []
        all_models = primary_models + validation_models
        return all_models[:self.config.max_models]
    
    def _get_model_timeout(self, model: str) -> int:
        """æ ¹æ®æ¨¡å‹ç±»å‹è·å–è¶…æ—¶æ—¶é—´"""
        # Premiumæ¨¡å‹éœ€è¦æ›´é•¿æ—¶é—´
        premium_models = ['gpt5-chat', 'claude-opus-41', 'gemini-25-pro', 'grok4']
        standard_models = ['gpt4o', 'claude', 'gemini']
        
        if model in premium_models:
            return 60  # Premiumæ¨¡å‹60ç§’
        elif model in standard_models:
            return 30  # æ ‡å‡†æ¨¡å‹30ç§’
        else:
            return 20  # ç»æµæ¨¡å‹20ç§’
    
    def _get_fallback_model(self, failed_model: str) -> Optional[str]:
        """è·å–å¤±è´¥æ¨¡å‹çš„é™çº§æ›¿ä»£"""
        fallback_map = {
            'gpt5-chat': 'gpt5-mini',
            'gpt5-mini': 'gpt4o',
            'claude-opus-41': 'claude',
            'claude': 'claude-haiku',
            'gemini-25-pro': 'gemini',
            'gemini': 'gemini-flash',
            'grok4': 'grok'
        }
        return fallback_map.get(failed_model)
    
    def _parallel_analyze(self, data: str, models: List[str], analysis_type: str) -> Dict[str, Any]:
        """å¹¶è¡Œè°ƒç”¨å¤šä¸ªæ¨¡å‹è¿›è¡Œåˆ†æ"""
        results = {}
        
        with ThreadPoolExecutor(max_workers=min(len(models), 5)) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_model = {
                executor.submit(self._safe_analyze_with_fallback, data, model, analysis_type): model 
                for model in models
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_model, timeout=self.config.timeout_seconds):
                model = future_to_model[future]
                try:
                    # ä½¿ç”¨æ¨¡å‹ç‰¹å®šçš„è¶…æ—¶æ—¶é—´
                    model_timeout = self._get_model_timeout(model)
                    result = future.result(timeout=model_timeout)
                    results[model] = result
                    if result.get('success'):
                        logger.info(f"âœ… {model} åˆ†æå®Œæˆ")
                    else:
                        logger.warning(f"âš ï¸ {model} åˆ†æå¤±è´¥ï¼Œå·²å°è¯•é™çº§å¤„ç†")
                except Exception as e:
                    logger.error(f"âŒ {model} åˆ†æå¤±è´¥: {e}")
                    results[model] = {
                        'success': False,
                        'error': str(e),
                        'model': model
                    }
        
        return results
    
    def _safe_analyze(self, data: str, model: str, analysis_type: str) -> Dict[str, Any]:
        """å®‰å…¨çš„å•æ¨¡å‹åˆ†æè°ƒç”¨"""
        try:
            result = self.client.analyze_market_data(
                data=data,
                model_name=model,
                analysis_type=analysis_type
            )
            return result
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'model': model
            }
    
    def _safe_analyze_with_fallback(self, data: str, model: str, analysis_type: str) -> Dict[str, Any]:
        """å¸¦é™çº§é‡è¯•çš„å®‰å…¨åˆ†æè°ƒç”¨"""
        try:
            # å°è¯•åŸå§‹æ¨¡å‹
            result = self.client.analyze_market_data(
                data=data,
                model_name=model,
                analysis_type=analysis_type
            )
            return result
        except Exception as e:
            logger.warning(f"ğŸ”„ {model} å¤±è´¥ï¼Œå°è¯•é™çº§: {e}")
            
            # å°è¯•é™çº§æ¨¡å‹
            fallback_model = self._get_fallback_model(model)
            if fallback_model:
                try:
                    logger.info(f"ğŸ”„ ä½¿ç”¨é™çº§æ¨¡å‹ {fallback_model}")
                    result = self.client.analyze_market_data(
                        data=data,
                        model_name=fallback_model,
                        analysis_type=analysis_type
                    )
                    # æ ‡è®°è¿™æ˜¯é™çº§ç»“æœ
                    result['fallback_used'] = True
                    result['original_model'] = model
                    result['fallback_model'] = fallback_model
                    return result
                except Exception as fallback_e:
                    logger.error(f"âŒ é™çº§æ¨¡å‹ {fallback_model} ä¹Ÿå¤±è´¥: {fallback_e}")
            
            return {
                'success': False,
                'error': str(e),
                'model': model,
                'fallback_attempted': fallback_model is not None
            }
    
    def _determine_confidence_level(self, consensus_score: float) -> str:
        """ç¡®å®šç½®ä¿¡åº¦ç­‰çº§"""
        if consensus_score >= 0.8:
            return 'high'
        elif consensus_score >= 0.6:
            return 'medium'
        else:
            return 'low'
    
    def _detect_disagreements(self, results: Dict[str, Any], consensus_score: float) -> List[str]:
        """æ£€æµ‹åˆ†æ­§ç‚¹"""
        disagreements = []
        
        if consensus_score < self.config.consensus_threshold:
            disagreements.append(f"å…±è¯†å¾—åˆ†è¿‡ä½: {consensus_score:.2f} < {self.config.consensus_threshold}")
        
        # æ£€æŸ¥å…·ä½“åˆ†æ­§ç‚¹
        disagreement_details = self.consensus_calc.identify_disagreements(results)
        disagreements.extend(disagreement_details)
        
        return disagreements
    
    def _execute_arbitration(self, data: str, analysis_type: str, disagreements: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œä»²è£åˆ†æ"""
        logger.info("ğŸ¤” æ£€æµ‹åˆ°é‡å¤§åˆ†æ­§ï¼Œå¯åŠ¨ä»²è£æœºåˆ¶...")
        
        # é€‰æ‹©ä»²è£æ¨¡å‹ï¼ˆé€šå¸¸é€‰æ‹©ä¸ä¸»è¦æ¨¡å‹ä¸åŒçš„æ¨¡å‹ï¼‰
        arbitrator = 'claude-haiku'  # ç®€æ´ä¸“ä¸šçš„ä»²è£è€…
        
        # æ„å»ºä»²è£æç¤º
        arbitration_prompt = f"""
ä½œä¸ºç‹¬ç«‹çš„ç¬¬ä¸‰æ–¹åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹åˆ†æ­§è¿›è¡Œä»²è£ï¼š

åˆ†æ­§ç‚¹ï¼š
{chr(10).join(disagreements)}

è¯·åŸºäºæ•°æ®è¿›è¡Œç‹¬ç«‹åˆ†æï¼Œå¹¶å¯¹äº‰è®®ç‚¹ç»™å‡ºæ˜ç¡®åˆ¤æ–­ã€‚
        """
        
        try:
            arbitration_result = self.client.analyze_market_data(
                data=data,
                model_name=arbitrator,
                analysis_type=analysis_type,
                custom_prompt=arbitration_prompt
            )
            
            logger.info("âœ… ä»²è£åˆ†æå®Œæˆ")
            return arbitration_result
            
        except Exception as e:
            logger.error(f"âŒ ä»²è£åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'error': f"ä»²è£å¤±è´¥: {str(e)}"
            }
    
    def _create_fallback_result(self, model_results: Dict[str, Any], start_time: float) -> ValidationResult:
        """åˆ›å»ºåå¤‡ç»“æœï¼ˆå½“éªŒè¯å¤±è´¥æ—¶ï¼‰"""
        processing_time = time.time() - start_time
        
        # å°è¯•æ‰¾åˆ°è‡³å°‘ä¸€ä¸ªæˆåŠŸçš„ç»“æœ
        successful_results = {k: v for k, v in model_results.items() if v.get('success', False)}
        
        return ValidationResult(
            consensus_score=0.0,
            confidence_level='low',
            primary_analysis=successful_results,
            validation_analyses={},
            disagreements=["éªŒè¯æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆäº¤å‰éªŒè¯"],
            processing_time=processing_time
        )
    
    def _log_validation_summary(self, result: ValidationResult):
        """è®°å½•éªŒè¯æ‘˜è¦"""
        logger.info("="*60)
        logger.info("ğŸ” å¤šæ¨¡å‹éªŒè¯ç»“æœæ‘˜è¦")
        logger.info(f"ğŸ“Š å…±è¯†å¾—åˆ†: {result.consensus_score:.2f}")
        logger.info(f"ğŸ¯ ç½®ä¿¡åº¦: {result.confidence_level.upper()}")
        logger.info(f"âœ… ä¸»è¦åˆ†æ: {len(result.primary_analysis)}ä¸ªæ¨¡å‹")
        logger.info(f"ğŸ”„ éªŒè¯åˆ†æ: {len(result.validation_analyses)}ä¸ªæ¨¡å‹")
        logger.info(f"âš ï¸ åˆ†æ­§æ•°é‡: {len(result.disagreements)}")
        logger.info(f"ğŸ’° æ€»æˆæœ¬: ${result.total_cost:.6f}")
        logger.info(f"â±ï¸ å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
        
        if result.disagreements:
            logger.warning("âš ï¸ æ£€æµ‹åˆ°çš„åˆ†æ­§:")
            for i, disagreement in enumerate(result.disagreements, 1):
                logger.warning(f"  {i}. {disagreement}")
        
        if result.arbitration_result:
            logger.info("ğŸ¤” å·²æ‰§è¡Œä»²è£åˆ†æ")
            
        logger.info("="*60)

    def get_validation_summary(self, result: ValidationResult) -> Dict[str, Any]:
        """è·å–éªŒè¯ç»“æœæ‘˜è¦ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰"""
        return {
            'consensus_score': result.consensus_score,
            'confidence_level': result.confidence_level,
            'model_count': {
                'primary': len(result.primary_analysis),
                'validation': len(result.validation_analyses),
                'total': len(result.primary_analysis) + len(result.validation_analyses)
            },
            'has_disagreements': len(result.disagreements) > 0,
            'disagreement_count': len(result.disagreements),
            'has_arbitration': result.arbitration_result is not None,
            'performance': {
                'total_cost': result.total_cost,
                'processing_time': result.processing_time
            },
            'recommendation': self._get_usage_recommendation(result)
        }
    
    def _get_usage_recommendation(self, result: ValidationResult) -> str:
        """è·å–ä½¿ç”¨å»ºè®®"""
        if result.confidence_level == 'high':
            return "åˆ†æç»“æœé«˜åº¦ä¸€è‡´ï¼Œå¯ä¿¡åº¦æé«˜ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨"
        elif result.confidence_level == 'medium':
            return "åˆ†æç»“æœåŸºæœ¬ä¸€è‡´ï¼Œå»ºè®®ç»“åˆäººå·¥åˆ¤æ–­ä½¿ç”¨"
        else:
            return "æ¨¡å‹é—´å­˜åœ¨æ˜¾è‘—åˆ†æ­§ï¼Œå¼ºçƒˆå»ºè®®äººå·¥å¤æ ¸åä½¿ç”¨"