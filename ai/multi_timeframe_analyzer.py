#!/usr/bin/env python3
"""
å¤šæ—¶é—´å‘¨æœŸAIåˆ†æå™¨ - ä¸“ä¸šçº§å¤šå‘¨æœŸåˆ†ææ ¸å¿ƒç»„ä»¶
åŸºäºç°æœ‰AIç›´æ¥åˆ†ææ¶æ„ï¼Œå®ç°æ™ºèƒ½å¤šæ—¶é—´æ¡†æ¶ç»¼åˆåˆ†æ
"""

import time
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
import logging
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import concurrent.futures
from threading import Lock

from .raw_data_analyzer import RawDataAnalyzer
from data import BinanceFetcher
from formatters import DataFormatter

logger = logging.getLogger(__name__)

class AnalysisScenario(Enum):
    """åˆ†æåœºæ™¯ç±»å‹"""
    INTRADAY_TRADING = "intraday"      # æ—¥å†…äº¤æ˜“
    TREND_ANALYSIS = "trend"           # è¶‹åŠ¿åˆ†æ  
    QUICK_CHECK = "quick"              # å¿«é€Ÿæ£€æŸ¥
    SWING_TRADING = "swing"            # æ³¢æ®µäº¤æ˜“
    POSITION_SIZING = "position"       # ä»“ä½ç®¡ç†

class VolatilityLevel(Enum):
    """æ³¢åŠ¨ç‡çº§åˆ«"""
    LOW = "low"          # ä½æ³¢åŠ¨
    NORMAL = "normal"    # æ­£å¸¸æ³¢åŠ¨
    HIGH = "high"        # é«˜æ³¢åŠ¨
    EXTREME = "extreme"  # æç«¯æ³¢åŠ¨

@dataclass
class TimeframeConfig:
    """æ—¶é—´æ¡†æ¶é…ç½®"""
    primary: str              # ä¸»è¦åˆ†æå‘¨æœŸ
    secondary: List[str]      # è¾…åŠ©åˆ†æå‘¨æœŸ
    update_frequency: int     # æ›´æ–°é¢‘ç‡ï¼ˆç§’ï¼‰
    data_limit: int          # æ•°æ®é‡é™åˆ¶
    weight: float = 1.0      # æƒé‡ç³»æ•°

@dataclass
class MultiTimeframeResult:
    """å¤šæ—¶é—´å‘¨æœŸåˆ†æç»“æœ"""
    scenario: AnalysisScenario
    primary_analysis: Dict[str, Any]
    secondary_analyses: Dict[str, Dict[str, Any]]
    consistency_score: float
    overall_signal: str
    risk_warnings: List[str]
    confidence_level: float
    execution_time: float
    timestamp: datetime = field(default_factory=datetime.now)

class ScenarioDetector:
    """æ™ºèƒ½åœºæ™¯è¯†åˆ«å™¨"""
    
    def __init__(self):
        self.volatility_threshold = {
            'high': 2.0,      # ATRå€æ•°
            'extreme': 3.5
        }
        self.volume_threshold = 3.0  # æˆäº¤é‡å€æ•°
        
    def detect_scenario(self, df: pd.DataFrame, user_intent: str = None) -> AnalysisScenario:
        """
        æ™ºèƒ½è¯†åˆ«åˆ†æåœºæ™¯
        
        Args:
            df: åŸå§‹OHLCVæ•°æ®
            user_intent: ç”¨æˆ·æ„å›¾ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯†åˆ«å‡ºçš„åˆ†æåœºæ™¯
        """
        if user_intent:
            intent_mapping = {
                'trade': AnalysisScenario.INTRADAY_TRADING,
                'trend': AnalysisScenario.TREND_ANALYSIS,
                'swing': AnalysisScenario.SWING_TRADING,
                'position': AnalysisScenario.POSITION_SIZING,
                'quick': AnalysisScenario.QUICK_CHECK
            }
            if user_intent.lower() in intent_mapping:
                return intent_mapping[user_intent.lower()]
        
        # è‡ªåŠ¨åœºæ™¯è¯†åˆ«
        volatility = self._calculate_volatility(df)
        volume_surge = self._detect_volume_surge(df)
        price_action = self._analyze_price_action(df)
        
        logger.info(f"åœºæ™¯è¯†åˆ« - æ³¢åŠ¨ç‡: {volatility.value}, æˆäº¤é‡å¼‚å¸¸: {volume_surge}, ä»·æ ¼è¡Œä¸º: {price_action}")
        
        # åœºæ™¯è¯†åˆ«é€»è¾‘
        if volatility in [VolatilityLevel.HIGH, VolatilityLevel.EXTREME] and volume_surge:
            return AnalysisScenario.INTRADAY_TRADING
        elif price_action in ['breakout', 'trend_change']:
            return AnalysisScenario.TREND_ANALYSIS
        elif price_action == 'range_bound':
            return AnalysisScenario.SWING_TRADING
        else:
            return AnalysisScenario.QUICK_CHECK
    
    def _calculate_volatility(self, df: pd.DataFrame) -> VolatilityLevel:
        """è®¡ç®—æ³¢åŠ¨ç‡çº§åˆ«"""
        if len(df) < 20:
            return VolatilityLevel.NORMAL
            
        # è®¡ç®—ATRï¼ˆç®€åŒ–ç‰ˆï¼‰
        high_low = (df['high'] - df['low']).rolling(14).mean()
        close_open = abs(df['close'] - df['open']).rolling(14).mean()
        current_atr = (high_low + close_open) / 2
        
        # å†å²ATRåŸºå‡†
        historical_atr = current_atr.rolling(50).mean()
        atr_ratio = current_atr.iloc[-1] / historical_atr.iloc[-1] if len(historical_atr) > 0 else 1.0
        
        if atr_ratio > self.volatility_threshold['extreme']:
            return VolatilityLevel.EXTREME
        elif atr_ratio > self.volatility_threshold['high']:
            return VolatilityLevel.HIGH
        elif atr_ratio < 0.7:
            return VolatilityLevel.LOW
        else:
            return VolatilityLevel.NORMAL
    
    def _detect_volume_surge(self, df: pd.DataFrame) -> bool:
        """æ£€æµ‹æˆäº¤é‡å¼‚å¸¸"""
        if len(df) < 20:
            return False
            
        avg_volume = df['volume'].rolling(20).mean()
        current_volume = df['volume'].iloc[-1]
        
        return current_volume > (avg_volume.iloc[-1] * self.volume_threshold)
    
    def _analyze_price_action(self, df: pd.DataFrame) -> str:
        """åˆ†æä»·æ ¼è¡Œä¸ºæ¨¡å¼"""
        if len(df) < 10:
            return 'insufficient_data'
            
        recent_data = df.tail(10)
        price_change = (recent_data['close'].iloc[-1] - recent_data['close'].iloc[0]) / recent_data['close'].iloc[0]
        
        # ç®€åŒ–çš„ä»·æ ¼è¡Œä¸ºè¯†åˆ«
        if abs(price_change) > 0.05:  # 5%ä»¥ä¸Šå˜åŒ–
            return 'breakout'
        elif abs(price_change) > 0.02:  # 2-5%å˜åŒ–
            return 'trend_change'
        else:
            return 'range_bound'

class TimeframeSelector:
    """æ™ºèƒ½æ—¶é—´æ¡†æ¶é€‰æ‹©å™¨"""
    
    # é¢„å®šä¹‰åœºæ™¯æ—¶é—´æ¡†æ¶é…ç½®
    SCENARIO_CONFIGS = {
        AnalysisScenario.INTRADAY_TRADING: TimeframeConfig(
            primary="15m",
            secondary=["5m", "1h", "4h"],
            update_frequency=300,  # 5åˆ†é’Ÿ
            data_limit=50
        ),
        AnalysisScenario.TREND_ANALYSIS: TimeframeConfig(
            primary="4h",
            secondary=["1h", "1d", "1w"],
            update_frequency=3600,  # 1å°æ—¶
            data_limit=100
        ),
        AnalysisScenario.SWING_TRADING: TimeframeConfig(
            primary="1h",
            secondary=["4h", "1d"],
            update_frequency=1800,  # 30åˆ†é’Ÿ
            data_limit=75
        ),
        AnalysisScenario.POSITION_SIZING: TimeframeConfig(
            primary="1d",
            secondary=["4h", "1w"],
            update_frequency=7200,  # 2å°æ—¶
            data_limit=50
        ),
        AnalysisScenario.QUICK_CHECK: TimeframeConfig(
            primary="1h",
            secondary=[],
            update_frequency=3600,  # 1å°æ—¶
            data_limit=30
        )
    }
    
    def select_timeframes(self, scenario: AnalysisScenario, custom_timeframes: List[str] = None) -> TimeframeConfig:
        """
        é€‰æ‹©æœ€ä¼˜æ—¶é—´æ¡†æ¶é…ç½®
        
        Args:
            scenario: åˆ†æåœºæ™¯
            custom_timeframes: ç”¨æˆ·è‡ªå®šä¹‰æ—¶é—´æ¡†æ¶
            
        Returns:
            æ—¶é—´æ¡†æ¶é…ç½®
        """
        if custom_timeframes:
            # ä½¿ç”¨è‡ªå®šä¹‰æ—¶é—´æ¡†æ¶
            primary = custom_timeframes[0]
            secondary = custom_timeframes[1:] if len(custom_timeframes) > 1 else []
            return TimeframeConfig(
                primary=primary,
                secondary=secondary,
                update_frequency=self._get_update_frequency(primary),
                data_limit=50
            )
        
        # ä½¿ç”¨é¢„å®šä¹‰é…ç½®
        config = self.SCENARIO_CONFIGS.get(scenario, self.SCENARIO_CONFIGS[AnalysisScenario.QUICK_CHECK])
        logger.info(f"é€‰æ‹©æ—¶é—´æ¡†æ¶é…ç½® - åœºæ™¯: {scenario.value}, ä¸»å‘¨æœŸ: {config.primary}, è¾…åŠ©å‘¨æœŸ: {config.secondary}")
        return config
    
    def _get_update_frequency(self, timeframe: str) -> int:
        """æ ¹æ®æ—¶é—´æ¡†æ¶è·å–æ›´æ–°é¢‘ç‡"""
        timeframe_to_seconds = {
            '1m': 60,
            '5m': 300,
            '15m': 900,
            '30m': 1800,
            '1h': 3600,
            '4h': 14400,
            '1d': 86400,
            '1w': 604800
        }
        return timeframe_to_seconds.get(timeframe, 3600)

class MultiTimeframeAnalyzer:
    """
    å¤šæ—¶é—´å‘¨æœŸAIåˆ†æå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ™ºèƒ½åœºæ™¯è¯†åˆ«å’Œæ—¶é—´æ¡†æ¶é€‰æ‹©
    2. å¹¶è¡Œå¤šå‘¨æœŸæ•°æ®è·å–å’ŒAIåˆ†æ
    3. åˆ†æç»“æœä¸€è‡´æ€§æ£€æŸ¥å’Œæ•´åˆ
    4. ä¸“ä¸šçº§é£é™©è¯„ä¼°å’Œä¿¡å·ç”Ÿæˆ
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """åˆå§‹åŒ–å¤šæ—¶é—´å‘¨æœŸåˆ†æå™¨"""
        self.raw_analyzer = RawDataAnalyzer(api_key)
        self.fetcher = BinanceFetcher()
        self.formatter = DataFormatter()
        
        self.scenario_detector = ScenarioDetector()
        self.timeframe_selector = TimeframeSelector()
        
        # ç»“æœç¼“å­˜
        self._cache = {}
        self._cache_lock = Lock()
        self._cache_expiry = 300  # 5åˆ†é’Ÿç¼“å­˜
        
        logger.info("âœ… å¤šæ—¶é—´å‘¨æœŸAIåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_multi_timeframe(self, 
                               symbol: str = "ETHUSDT",
                               model: str = "gemini-flash",
                               analysis_type: str = "complete",
                               analysis_method: Optional[str] = None,
                               scenario: Optional[AnalysisScenario] = None,
                               custom_timeframes: Optional[List[str]] = None,
                               user_intent: Optional[str] = None) -> MultiTimeframeResult:
        """
        æ‰§è¡Œå¤šæ—¶é—´å‘¨æœŸç»¼åˆåˆ†æ
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            model: AIæ¨¡å‹
            analysis_type: åˆ†æç±»å‹
            analysis_method: åˆ†ææ–¹æ³•
            scenario: æŒ‡å®šåˆ†æåœºæ™¯
            custom_timeframes: è‡ªå®šä¹‰æ—¶é—´æ¡†æ¶
            user_intent: ç”¨æˆ·æ„å›¾
            
        Returns:
            å¤šæ—¶é—´å‘¨æœŸåˆ†æç»“æœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹å¤šæ—¶é—´å‘¨æœŸåˆ†æ - {symbol}, æ¨¡å‹: {model}")
            
            # 1. è·å–ä¸»è¦å‘¨æœŸæ•°æ®ç”¨äºåœºæ™¯è¯†åˆ«
            primary_timeframe = custom_timeframes[0] if custom_timeframes else "1h"
            primary_df = self._get_data_with_cache(symbol, primary_timeframe, 100)
            
            # 2. æ™ºèƒ½åœºæ™¯è¯†åˆ«
            if not scenario:
                scenario = self.scenario_detector.detect_scenario(primary_df, user_intent)
            logger.info(f"ğŸ“Š è¯†åˆ«åˆ†æåœºæ™¯: {scenario.value}")
            
            # 3. æ™ºèƒ½æ—¶é—´æ¡†æ¶é€‰æ‹©
            timeframe_config = self.timeframe_selector.select_timeframes(scenario, custom_timeframes)
            
            # 4. å¹¶è¡Œè·å–å¤šå‘¨æœŸæ•°æ®å’Œåˆ†æ
            all_analyses = self._execute_parallel_analysis(
                symbol=symbol,
                timeframe_config=timeframe_config,
                model=model,
                analysis_type=analysis_type,
                analysis_method=analysis_method
            )
            
            # 5. åˆ†æç»“æœæ•´åˆå’Œä¸€è‡´æ€§æ£€æŸ¥
            result = self._integrate_analyses(
                scenario=scenario,
                timeframe_config=timeframe_config,
                analyses=all_analyses,
                execution_time=time.time() - start_time
            )
            
            logger.info(f"âœ… å¤šæ—¶é—´å‘¨æœŸåˆ†æå®Œæˆ - è€—æ—¶: {result.execution_time:.2f}ç§’, ä¸€è‡´æ€§: {result.consistency_score:.1f}/100")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¤šæ—¶é—´å‘¨æœŸåˆ†æå¤±è´¥: {e}")
            # è¿”å›é”™è¯¯ç»“æœ
            return MultiTimeframeResult(
                scenario=scenario or AnalysisScenario.QUICK_CHECK,
                primary_analysis={'error': str(e), 'success': False},
                secondary_analyses={},
                consistency_score=0.0,
                overall_signal="ERROR",
                risk_warnings=[f"åˆ†æå¤±è´¥: {str(e)}"],
                confidence_level=0.0,
                execution_time=time.time() - start_time
            )
    
    def _get_data_with_cache(self, symbol: str, timeframe: str, limit: int) -> pd.DataFrame:
        """å¸¦ç¼“å­˜çš„æ•°æ®è·å–"""
        cache_key = f"{symbol}_{timeframe}_{limit}"
        
        with self._cache_lock:
            if cache_key in self._cache:
                cached_data, cache_time = self._cache[cache_key]
                if time.time() - cache_time < self._cache_expiry:
                    return cached_data
        
        # è·å–æ–°æ•°æ®
        symbol_for_api = symbol.replace('USDT', '/USDT') if '/' not in symbol else symbol
        df = self.fetcher.get_ohlcv(symbol_for_api, timeframe, limit)
        
        # ç¼“å­˜æ•°æ®
        with self._cache_lock:
            self._cache[cache_key] = (df, time.time())
        
        return df
    
    def _execute_parallel_analysis(self, 
                                  symbol: str,
                                  timeframe_config: TimeframeConfig,
                                  model: str,
                                  analysis_type: str,
                                  analysis_method: Optional[str]) -> Dict[str, Dict[str, Any]]:
        """å¹¶è¡Œæ‰§è¡Œå¤šå‘¨æœŸåˆ†æ"""
        
        all_timeframes = [timeframe_config.primary] + timeframe_config.secondary
        analyses = {}
        
        def analyze_single_timeframe(timeframe: str) -> Tuple[str, Dict[str, Any]]:
            """åˆ†æå•ä¸ªæ—¶é—´æ¡†æ¶"""
            try:
                df = self._get_data_with_cache(symbol, timeframe, timeframe_config.data_limit)
                
                result = self.raw_analyzer.analyze_raw_ohlcv(
                    df=df,
                    model=model,
                    analysis_type=analysis_type,
                    analysis_method=analysis_method
                )
                
                result['timeframe'] = timeframe
                result['data_points'] = len(df)
                return timeframe, result
                
            except Exception as e:
                logger.error(f"âŒ åˆ†ææ—¶é—´æ¡†æ¶ {timeframe} å¤±è´¥: {e}")
                return timeframe, {
                    'error': str(e),
                    'success': False,
                    'timeframe': timeframe
                }
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œåˆ†æ
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            future_to_timeframe = {
                executor.submit(analyze_single_timeframe, tf): tf 
                for tf in all_timeframes
            }
            
            for future in concurrent.futures.as_completed(future_to_timeframe):
                timeframe, result = future.result()
                analyses[timeframe] = result
        
        return analyses
    
    def _integrate_analyses(self, 
                           scenario: AnalysisScenario,
                           timeframe_config: TimeframeConfig,
                           analyses: Dict[str, Dict[str, Any]],
                           execution_time: float) -> MultiTimeframeResult:
        """æ•´åˆåˆ†æç»“æœ"""
        
        primary_analysis = analyses.get(timeframe_config.primary, {})
        secondary_analyses = {
            tf: analysis for tf, analysis in analyses.items() 
            if tf != timeframe_config.primary
        }
        
        # è®¡ç®—ä¸€è‡´æ€§è¯„åˆ†
        consistency_score = self._calculate_consistency_score(analyses)
        
        # ç”Ÿæˆç»¼åˆä¿¡å·
        overall_signal = self._generate_overall_signal(analyses, consistency_score)
        
        # è¯†åˆ«é£é™©è­¦å‘Š
        risk_warnings = self._identify_risk_warnings(analyses, consistency_score)
        
        # è®¡ç®—ä¿¡å¿ƒæ°´å¹³
        confidence_level = self._calculate_confidence_level(analyses, consistency_score)
        
        return MultiTimeframeResult(
            scenario=scenario,
            primary_analysis=primary_analysis,
            secondary_analyses=secondary_analyses,
            consistency_score=consistency_score,
            overall_signal=overall_signal,
            risk_warnings=risk_warnings,
            confidence_level=confidence_level,
            execution_time=execution_time
        )
    
    def _calculate_consistency_score(self, analyses: Dict[str, Dict[str, Any]]) -> float:
        """è®¡ç®—åˆ†æä¸€è‡´æ€§è¯„åˆ†"""
        if len(analyses) < 2:
            return 100.0
            
        successful_analyses = [a for a in analyses.values() if a.get('success', False)]
        if len(successful_analyses) < 2:
            return 0.0
        
        # ç®€åŒ–çš„ä¸€è‡´æ€§è®¡ç®—ï¼ˆåŸºäºè´¨é‡è¯„åˆ†ç›¸ä¼¼åº¦ï¼‰
        quality_scores = [a.get('quality_score', 50) for a in successful_analyses if 'quality_score' in a]
        
        if len(quality_scores) < 2:
            return 70.0  # é»˜è®¤ä¸­ç­‰ä¸€è‡´æ€§
            
        # è®¡ç®—è¯„åˆ†æ ‡å‡†å·®ï¼Œè½¬æ¢ä¸ºä¸€è‡´æ€§åˆ†æ•°
        import statistics
        std_dev = statistics.stdev(quality_scores)
        consistency = max(0, 100 - (std_dev * 2))  # æ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
        
        return min(100.0, consistency)
    
    def _generate_overall_signal(self, analyses: Dict[str, Dict[str, Any]], consistency_score: float) -> str:
        """ç”Ÿæˆç»¼åˆäº¤æ˜“ä¿¡å·"""
        successful_analyses = [a for a in analyses.values() if a.get('success', False)]
        
        if not successful_analyses:
            return "NO_SIGNAL"
            
        if consistency_score > 80:
            return "STRONG_SIGNAL" 
        elif consistency_score > 60:
            return "MODERATE_SIGNAL"
        elif consistency_score > 40:
            return "WEAK_SIGNAL"
        else:
            return "CONFLICTING_SIGNALS"
    
    def _identify_risk_warnings(self, analyses: Dict[str, Dict[str, Any]], consistency_score: float) -> List[str]:
        """è¯†åˆ«é£é™©è­¦å‘Š"""
        warnings = []
        
        if consistency_score < 50:
            warnings.append("å¤šæ—¶é—´æ¡†æ¶ä¿¡å·ä¸¥é‡å†²çªï¼Œå»ºè®®è°¨æ…äº¤æ˜“")
            
        failed_count = sum(1 for a in analyses.values() if not a.get('success', False))
        if failed_count > 0:
            warnings.append(f"{failed_count}ä¸ªæ—¶é—´æ¡†æ¶åˆ†æå¤±è´¥")
            
        # æ£€æŸ¥æ•°æ®è´¨é‡
        low_quality_count = sum(1 for a in analyses.values() 
                               if a.get('success', False) and a.get('quality_score', 100) < 60)
        if low_quality_count > 0:
            warnings.append(f"{low_quality_count}ä¸ªæ—¶é—´æ¡†æ¶åˆ†æè´¨é‡è¾ƒä½")
            
        return warnings
    
    def _calculate_confidence_level(self, analyses: Dict[str, Dict[str, Any]], consistency_score: float) -> float:
        """è®¡ç®—æ•´ä½“ä¿¡å¿ƒæ°´å¹³"""
        successful_analyses = [a for a in analyses.values() if a.get('success', False)]
        
        if not successful_analyses:
            return 0.0
            
        # åŸºäºæˆåŠŸç‡ã€ä¸€è‡´æ€§å’Œè´¨é‡è¯„åˆ†è®¡ç®—ä¿¡å¿ƒæ°´å¹³
        success_rate = len(successful_analyses) / len(analyses)
        avg_quality = sum(a.get('quality_score', 50) for a in successful_analyses) / len(successful_analyses)
        
        confidence = (success_rate * 40 + consistency_score * 0.4 + avg_quality * 0.2)
        return min(100.0, confidence)
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._cache_lock:
            self._cache.clear()
        logger.info("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º")