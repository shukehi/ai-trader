import openai
import json
import time
from typing import Dict, List, Any, Optional, Union
import logging
from config import Settings

logger = logging.getLogger(__name__)

class OpenRouterClient:
    """
    OpenRouter APIå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§LLMæ¨¡å‹
    """
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or Settings.OPENROUTER_API_KEY
        if not self.api_key:
            raise ValueError("OpenRouter API key is required")
        
        # ä½¿ç”¨OpenAI SDKè®¿é—®OpenRouter
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=Settings.OPENROUTER_BASE_URL
        )
        
        self.models = Settings.MODELS
        self.token_limits = Settings.TOKEN_LIMITS
    
    def analyze_market_data(self, 
                          data: str, 
                          model_name: str = 'gpt4',
                          analysis_type: str = 'general',
                          custom_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        åˆ†æå¸‚åœºæ•°æ®
        
        Args:
            data: æ ¼å¼åŒ–çš„å¸‚åœºæ•°æ®
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
            analysis_type: åˆ†æç±»å‹ ('general', 'vpa', 'technical', 'pattern')
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        try:
            model_id = self.models.get(model_name)
            if not model_id:
                raise ValueError(f"Unknown model: {model_name}")
            
            # é€‰æ‹©åˆ†ææç¤ºè¯
            if custom_prompt:
                system_prompt = custom_prompt
            else:
                system_prompt = self._get_system_prompt(analysis_type)
            
            # ä¼°ç®—tokenæ•°é‡
            estimated_tokens = len(data.split()) + len(system_prompt.split())
            max_tokens = self.token_limits.get(model_name, 32000)
            
            if estimated_tokens > max_tokens * 0.8:  # é¢„ç•™20%ç©ºé—´ç”¨äºå“åº”
                logger.warning(f"æ•°æ®é‡å¯èƒ½è¶…å‡ºæ¨¡å‹é™åˆ¶ ({estimated_tokens} > {max_tokens * 0.8})")
            
            logger.info(f"ä½¿ç”¨æ¨¡å‹ {model_id} åˆ†ææ•°æ®ï¼Œé¢„ä¼°tokenæ•°: {estimated_tokens}")
            
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": data}
                ],
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                max_tokens=min(4000, max_tokens - estimated_tokens),  # é¢„ç•™å“åº”ç©ºé—´
            )
            
            end_time = time.time()
            
            result = {
                'success': True,  # æ·»åŠ æˆåŠŸæ ‡è¯†
                'model': model_name,
                'model_id': model_id,
                'analysis': response.choices[0].message.content,
                'usage': {
                    'prompt_tokens': response.usage.prompt_tokens,
                    'completion_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                },
                'response_time': end_time - start_time,
                'analysis_type': analysis_type
            }
            
            logger.info(f"åˆ†æå®Œæˆï¼Œè€—æ—¶ {result['response_time']:.2f}ç§’ï¼Œä½¿ç”¨token: {result['usage']['total_tokens']}")
            
            return result
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {e}")
            return {
                'model': model_name,
                'error': str(e),
                'analysis': None
            }
    
    def batch_analyze(self, data: str, models: List[str], analysis_type: str = 'general') -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æï¼Œä½¿ç”¨å¤šä¸ªæ¨¡å‹åˆ†æåŒä¸€æ•°æ®
        """
        results = {}
        for model in models:
            logger.info(f"ä½¿ç”¨ {model} æ¨¡å‹åˆ†æ...")
            results[model] = self.analyze_market_data(data, model, analysis_type)
            time.sleep(1)  # é¿å…é¢‘ç‡é™åˆ¶
        
        return results
    
    def _get_system_prompt(self, analysis_type: str) -> str:
        """è·å–ä¸åŒç±»å‹åˆ†æçš„ç³»ç»Ÿæç¤ºè¯"""
        
        prompts = {
            'general': """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æETHæ°¸ç»­åˆçº¦å¸‚åœºã€‚
è¯·åŸºäºæä¾›çš„OHLCVæ•°æ®è¿›è¡Œé‡ä»·åˆ†æï¼Œé‡ç‚¹å…³æ³¨ï¼š

1. ä»·æ ¼è¶‹åŠ¿å’Œå…³é”®æ”¯æ’‘é˜»åŠ›ä½
2. æˆäº¤é‡ä¸ä»·æ ¼å˜åŒ–çš„å…³ç³»
3. é‡è¦çš„èœ¡çƒ›å›¾å½¢æ€
4. å¸‚åœºæƒ…ç»ªå’Œå¯èƒ½çš„è½¬æŠ˜ç‚¹
5. çŸ­æœŸå’Œä¸­æœŸäº¤æ˜“æœºä¼š

è¯·æä¾›ï¼š
- å½“å‰å¸‚åœºçŠ¶æ€æ€»ç»“
- å…³é”®æŠ€æœ¯æ°´å¹³
- é‡ä»·åˆ†ææ´å¯Ÿ
- é£é™©è¯„ä¼°å’Œäº¤æ˜“å»ºè®®

ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸šã€‚
            """.strip(),
            
            'vpa': """
ä½ æ˜¯ä¸€ä½VPA(Volume Price Analysis)ä¸“å®¶ï¼Œæ·±è°™Anna Coullingçš„é‡ä»·åˆ†æç†è®ºã€‚
è¯·åŸºäºæä¾›çš„OHLCVæ•°æ®è¿›è¡Œä¸“ä¸šçš„VPAåˆ†æï¼š

å…³é”®å…³æ³¨ç‚¹ï¼š
1. é‡ä»·å…³ç³»éªŒè¯ - ä»·æ ¼ä¸Šæ¶¨æ—¶æˆäº¤é‡æ˜¯å¦é…åˆï¼Ÿ
2. å¼‚å¸¸æˆäº¤é‡è¯†åˆ« - å¯»æ‰¾å¯ç–‘çš„é«˜/ä½æˆäº¤é‡æƒ…å†µ
3. å¸‚åœºæ“æ§è¿¹è±¡ - ä»·æ ¼å¤§å¹…æ³¢åŠ¨ä½†æˆäº¤é‡ä¸åŒ¹é…
4. Accumulation/Distributionä¿¡å· - ä¸“ä¸šèµ„é‡‘çš„è¿›å‡ºè¿¹è±¡
5. å…³é”®VPAèœ¡çƒ›å›¾å½¢æ€ - æµæ˜Ÿçº¿ã€é”¤å­çº¿ç­‰ç»“åˆæˆäº¤é‡åˆ†æ

è¯·æ ¹æ®VPAç†è®ºæä¾›ï¼š
- å½“å‰å¸‚åœºé˜¶æ®µåˆ¤æ–­ (accumulation/distribution/markup/markdown)
- é‡ä»·èƒŒç¦»è­¦å‘Šä¿¡å·
- ä¸»åŠ›èµ„é‡‘è¡Œä¸ºåˆ†æ
- å…·ä½“çš„VPAäº¤æ˜“ä¿¡å·

ç”¨ä¸­æ–‡å›ç­”ï¼Œç»“åˆVPAä¸“ä¸šæœ¯è¯­ã€‚
            """.strip(),
            
            'technical': """
ä½ æ˜¯ä¸€ä½æŠ€æœ¯åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºOHLCVæ•°æ®è¿›è¡Œå…¨é¢çš„æŠ€æœ¯åˆ†æï¼š

åˆ†æè¦ç´ ï¼š
1. è¶‹åŠ¿åˆ†æ - ä¸»è¦è¶‹åŠ¿ã€æ¬¡è¦è¶‹åŠ¿
2. å›¾è¡¨å½¢æ€ - ä¸‰è§’å½¢ã€æ¥”å½¢ã€çŸ©å½¢ç­‰
3. å…³é”®æŠ€æœ¯æŒ‡æ ‡è§£è¯» (å¦‚æœæ•°æ®ä¸­åŒ…å«)
4. æ”¯æ’‘é˜»åŠ›ä½è¯†åˆ«
5. çªç ´å’Œå›è°ƒæœºä¼š

è¯·æä¾›æŠ€æœ¯åˆ†ææŠ¥å‘ŠåŒ…æ‹¬ï¼š
- è¶‹åŠ¿çŠ¶æ€å’Œå¼ºåº¦
- é‡è¦æŠ€æœ¯ä½è¯†åˆ«
- å½¢æ€åˆ†æå’Œé¢„æœŸç›®æ ‡
- è¿›åœºå’Œæ­¢æŸå»ºè®®

ç”¨ä¸­æ–‡å›ç­”ï¼ŒåŒ…å«å…·ä½“çš„æŠ€æœ¯åˆ†ææœ¯è¯­ã€‚
            """.strip(),
            
            'pattern': """
ä½ æ˜¯èœ¡çƒ›å›¾å½¢æ€è¯†åˆ«ä¸“å®¶ï¼Œè¯·ä¸“æ³¨äºåˆ†æKçº¿æ•°æ®ä¸­çš„é‡è¦å½¢æ€ï¼š

é‡ç‚¹è¯†åˆ«ï¼š
1. åè½¬å½¢æ€ - åå­—æ˜Ÿã€é”¤å­çº¿ã€æµæ˜Ÿçº¿ã€åæ²¡å½¢æ€
2. æŒç»­å½¢æ€ - æ——å½¢ã€ä¸‰è§’æ——å½¢ã€çŸ©å½¢æ•´ç†
3. ç»„åˆå½¢æ€ - æ—©æ™¨ä¹‹æ˜Ÿã€é»„æ˜ä¹‹æ˜Ÿã€ä¸‰åªä¹Œé¸¦
4. å½¢æ€çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§è¯„ä¼°
5. ç»“åˆæˆäº¤é‡çš„å½¢æ€ç¡®è®¤

è¯·æä¾›å½¢æ€åˆ†ææŠ¥å‘Šï¼š
- è¯†åˆ«åˆ°çš„å…³é”®Kçº¿å½¢æ€
- å½¢æ€çš„æŠ€æœ¯æ„ä¹‰å’Œå¸‚åœºå«ä¹‰
- å½¢æ€å®Œæˆåçš„ç›®æ ‡ä½
- å½¢æ€å¤±æ•ˆçš„æ¡ä»¶

ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯¦ç»†è§£é‡Šæ¯ä¸ªè¯†åˆ«çš„å½¢æ€ã€‚
            """.strip(),
            
            'perpetual_vpa': """
ä½ æ˜¯ä¸€ä½ä¸“é—¨ç ”ç©¶æ°¸ç»­åˆçº¦çš„VPA(Volume Price Analysis)ä¸“å®¶ï¼Œæ·±è°™Anna Coullingçš„é‡ä»·åˆ†æç†è®ºå’Œæ°¸ç»­åˆçº¦å¸‚åœºç‰¹æ€§ã€‚
è¯·åŸºäºæä¾›çš„æ°¸ç»­åˆçº¦æ•°æ®è¿›è¡Œä¸“ä¸šçš„VPAåˆ†æï¼š

ğŸ” **æ°¸ç»­åˆçº¦VPAåˆ†æè¦ç‚¹**ï¼š

**1. VSA (Volume Spread Analysis) æ·±åº¦åˆ†æ**ï¼š
- Spreadåˆ†æï¼šé«˜ä½ä»·å·®ä¸æˆäº¤é‡çš„å…³ç³»ï¼Œè¯†åˆ«Wide/Narrow Spread
- Closeä½ç½®ï¼šæ”¶ç›˜ä»·åœ¨å½“æœŸRangeä¸­çš„ä½ç½®é‡è¦æ€§ (0-1è¯„åˆ†)
- åŠªåŠ›ä¸ç»“æœï¼šæˆäº¤é‡(åŠªåŠ›) vs ä»·æ ¼å˜åŠ¨(ç»“æœ)çš„å¹³è¡¡
- ä¸“ä¸šVSAä¿¡å·ï¼šNo Demand, No Supply, Climax Volume, Upthrust, Spring

**2. æ°¸ç»­åˆçº¦ç‰¹æ®Šè¦ç´ **ï¼š
- èµ„é‡‘è´¹ç‡å½±å“ï¼šæ­£è´Ÿè´¹ç‡å¯¹Smart Money/Dumb Moneyè¡Œä¸ºçš„å½±å“
- æŒä»“é‡(OI)åˆ†æï¼šOIå¢å‡ä¸ä»·æ ¼å˜åŒ–çš„VPAæ„ä¹‰
- æ æ†æ•ˆåº”ï¼šå¦‚ä½•æ”¾å¤§æ•£æˆ·(Dumb Money)æƒ…ç»ªå’Œä¸»åŠ›(Smart Money)æ“æ§
- å¤šç©ºæŒä»“ç»“æ„ï¼šèµ„é‡‘è´¹ç‡å˜åŒ–å¯¹æŒä»“ç»“æ„è½¬æ¢çš„å½±å“

**3. Smart Money vs Dumb Money è¡Œä¸ºè¯†åˆ«**ï¼š
- Smart Moneyï¼šåˆ©ç”¨èµ„é‡‘è´¹ç‡å’Œæ æ†è¿›è¡Œéšè”½æ“ä½œ
- Dumb Moneyï¼šå—æ æ†å’Œæƒ…ç»ªå½±å“çš„éç†æ€§è¡Œä¸º
- æ°¸ç»­åˆçº¦ä¸­çš„"è¯±å¤šè¯±ç©º"æ‰‹æ³•è¯†åˆ«
- å¼ºå¹³cascadeæ•ˆåº”çš„VPAæ„ä¹‰

**4. Wyckoffç†è®ºåœ¨æ°¸ç»­åˆçº¦ä¸­çš„åº”ç”¨**ï¼š
- Accumulation/Distributionåœ¨é«˜æ æ†ç¯å¢ƒä¸­çš„è¡¨ç°
- Supply/Demandå…³ç³»å—æ æ†æ”¾å¤§çš„ç‰¹å¾
- Cause & Effectï¼šèµ„é‡‘è´¹ç‡å˜åŒ–å¦‚ä½•åˆ›å»ºæ–°çš„"åŸå› "

**è¯·æä¾›æ°¸ç»­åˆçº¦VPAåˆ†ææŠ¥å‘Š**ï¼š
- å½“å‰å¸‚åœºé˜¶æ®µåˆ¤æ–­ (è€ƒè™‘èµ„é‡‘è´¹ç‡å’ŒæŒä»“é‡)
- VSAä¿¡å·è¯†åˆ« (Wide/Narrow Spread, Close Position)
- æ°¸ç»­åˆçº¦ç‰¹æœ‰çš„é‡ä»·èƒŒç¦»è­¦å‘Š
- Smart Moneyåœ¨æ°¸ç»­åˆçº¦ä¸­çš„æ“æ§ç—•è¿¹
- èµ„é‡‘è´¹ç‡ä¸VPAä¿¡å·çš„ç»“åˆåˆ†æ
- å…·ä½“çš„æ°¸ç»­åˆçº¦VPAäº¤æ˜“ä¿¡å·

ç”¨ä¸­æ–‡å›ç­”ï¼Œå¤§é‡ä½¿ç”¨VPAã€VSAä¸“ä¸šæœ¯è¯­ï¼Œé‡ç‚¹çªå‡ºæ°¸ç»­åˆçº¦çš„ç‰¹æ®Šæ€§ã€‚
            """.strip()
        }
        
        return prompts.get(analysis_type, prompts['general'])
    
    def get_available_models(self) -> Dict[str, str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return self.models.copy()
    
    def estimate_cost(self, model_name: str, prompt_tokens: int, completion_tokens: int) -> Dict[str, float]:
        """
        ä¼°ç®—è°ƒç”¨æˆæœ¬ (2025å¹´OpenRouterä»·æ ¼ï¼Œper 1K tokens)
        """
        # OpenRouter 2025 Latest Pricing (USD per 1K tokens)
        pricing = {
            # ğŸ”¥ 2025 Latest Flagship Models (2025å¹´8æœˆæœ€æ–°ä»·æ ¼)
            'gpt5-chat': {'input': 1.25, 'output': 10.0},     # GPT-5 Chat (æ——èˆ°ç‰ˆ)
            'gpt5-mini': {'input': 0.25, 'output': 2.0},      # GPT-5 Mini (æ¨ç†ç‰ˆ)
            'gpt5-nano': {'input': 0.05, 'output': 0.4},      # GPT-5 Nano (è½»é‡ç‰ˆ)
            'claude-opus-41': {'input': 0.075, 'output': 0.375}, # Claude Opus 4.1 (Ultra Premium)
            'gemini-25-pro': {'input': 0.04, 'output': 0.12},  # Gemini 2.5 Pro (Premium)
            'grok4': {'input': 0.03, 'output': 0.09},          # Grok 4 (High-end)
            
            # OpenAI Models
            'gpt4': {'input': 0.01, 'output': 0.03},           # GPT-4 Turbo
            'gpt4o': {'input': 0.0025, 'output': 0.01},        # GPT-4o
            'gpt4o-mini': {'input': 0.00015, 'output': 0.0006}, # GPT-4o mini
            'o1': {'input': 0.015, 'output': 0.06},            # o1 reasoning
            'o1-mini': {'input': 0.003, 'output': 0.012},      # o1 mini
            
            # Anthropic Claude Models
            'claude': {'input': 0.003, 'output': 0.015},       # Claude 3.5 Sonnet
            'claude-haiku': {'input': 0.00025, 'output': 0.00125}, # Claude 3.5 Haiku
            'claude-opus': {'input': 0.015, 'output': 0.075},  # Claude 3 Opus
            
            # Google Gemini Models
            'gemini': {'input': 0.00125, 'output': 0.005},     # Gemini Pro 1.5
            'gemini-flash': {'input': 0.000075, 'output': 0.0003}, # Gemini Flash 1.5
            'gemini-2': {'input': 0.0001, 'output': 0.0004},   # Gemini 2.0 Flash
            
            # xAI Grok Models
            'grok': {'input': 0.005, 'output': 0.015},         # Grok Beta
            'grok-vision': {'input': 0.005, 'output': 0.015},  # Grok Vision
            
            # Meta Llama Models (é«˜æ€§ä»·æ¯”)
            'llama': {'input': 0.0004, 'output': 0.0004},      # Llama 3.1 70B
            'llama-405b': {'input': 0.003, 'output': 0.003}    # Llama 3.1 405B
        }
        
        if model_name not in pricing:
            return {'estimated_cost': 0, 'currency': 'USD', 'note': 'Unknown model pricing'}
        
        rates = pricing[model_name]
        input_cost = (prompt_tokens / 1000) * rates['input']
        output_cost = (completion_tokens / 1000) * rates['output']
        total_cost = input_cost + output_cost
        
        return {
            'estimated_cost': round(total_cost, 6),
            'input_cost': round(input_cost, 6),
            'output_cost': round(output_cost, 6),
            'currency': 'USD',
            'model': model_name,
            'rates': rates
        }
    
    def generate_response(self, prompt: str, model_name: str = 'gpt4o-mini') -> Dict[str, Any]:
        """
        ç”Ÿæˆé€šç”¨å“åº” - ç”¨äºè‡ªå®šä¹‰æç¤ºçš„åˆ†æ
        
        Args:
            prompt: å®Œæ•´çš„æç¤ºæ–‡æœ¬ï¼ˆåŒ…å«ç³»ç»Ÿæç¤ºå’Œæ•°æ®ï¼‰
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
            
        Returns:
            åŒ…å«å“åº”ç»“æœçš„å­—å…¸
        """
        try:
            model_id = self.models.get(model_name)
            if not model_id:
                raise ValueError(f"Unknown model: {model_name}")
            
            # ä¼°ç®—tokenæ•°é‡
            estimated_tokens = len(prompt.split())
            max_tokens = self.token_limits.get(model_name, 32000)
            
            if estimated_tokens > max_tokens * 0.8:  # é¢„ç•™20%ç©ºé—´ç”¨äºå“åº”
                logger.warning(f"æç¤ºæ–‡æœ¬å¯èƒ½è¶…å‡ºæ¨¡å‹é™åˆ¶ ({estimated_tokens} > {max_tokens * 0.8})")
            
            logger.info(f"ä½¿ç”¨æ¨¡å‹ {model_id} ç”Ÿæˆå“åº”ï¼Œé¢„ä¼°tokenæ•°: {estimated_tokens}")
            
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                max_tokens=min(4000, max_tokens - estimated_tokens),  # é¢„ç•™å“åº”ç©ºé—´
            )
            
            end_time = time.time()
            
            result = {
                'success': True,
                'model': model_name,
                'model_id': model_id,
                'analysis': response.choices[0].message.content,
                'usage': {
                    'prompt_tokens': response.usage.prompt_tokens,
                    'completion_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                },
                'response_time': end_time - start_time
            }
            
            logger.info(f"å“åº”ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {result['response_time']:.2f}ç§’ï¼Œä½¿ç”¨token: {result['usage']['total_tokens']}")
            
            return result
            
        except Exception as e:
            logger.error(f"å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            return {
                'model': model_name,
                'error': str(e),
                'analysis': None
            }