#!/usr/bin/env python3
"""
2025å¹´æœ€æ–°æ¨¡å‹æµ‹è¯•
æµ‹è¯•OpenAI GPT-4oã€Claude 3.5ã€Gemini 2.0ã€Grokç­‰æœ€æ–°æ¨¡å‹çš„æ€§èƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from datetime import datetime
from data import BinanceFetcher, DataProcessor
from formatters import DataFormatter
from ai import OpenRouterClient
from config import Settings
import logging
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Model2025Tester:
    def __init__(self):
        try:
            Settings.validate()
            self.fetcher = BinanceFetcher()
            self.formatter = DataFormatter()
            self.client = OpenRouterClient()
            logger.info("âœ… 2025æ¨¡å‹æµ‹è¯•å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def get_test_models(self):
        """è·å–è¦æµ‹è¯•çš„æ¨¡å‹åˆ†ç»„"""
        return {
            'flagship': ['gpt5', 'claude-opus-41', 'gemini-25-pro', 'grok4'],  # ğŸ”¥ 2025æœ€æ–°æ——èˆ°ç‰ˆ
            'economy': ['gpt4o-mini', 'claude-haiku', 'gemini-flash', 'llama'],
            'balanced': ['gpt4o', 'claude', 'gemini', 'grok'],
            'premium': ['o1-mini', 'claude-opus', 'gemini-2', 'llama-405b'],
            'reasoning': ['o1', 'o1-mini']  # ä¸“é—¨çš„æ¨ç†æ¨¡å‹
        }
    
    def test_model_performance(self, model_name: str, data: str) -> dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
        logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
        
        start_time = time.time()
        
        try:
            result = self.client.analyze_market_data(
                data=data,
                model_name=model_name,
                analysis_type='vpa'
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            if result.get('error'):
                return {
                    'model': model_name,
                    'success': False,
                    'error': result['error'],
                    'response_time': response_time
                }
            
            # åˆ†æç»“æœè´¨é‡æŒ‡æ ‡
            analysis = result.get('analysis', '')
            
            performance_metrics = {
                'model': model_name,
                'success': True,
                'response_time': response_time,
                'token_usage': result.get('usage', {}),
                'analysis_length': len(analysis),
                'analysis_quality_score': self._assess_analysis_quality(analysis),
                'cost_estimate': self.client.estimate_cost(
                    model_name, 
                    result.get('usage', {}).get('prompt_tokens', 0),
                    result.get('usage', {}).get('completion_tokens', 0)
                )
            }
            
            logger.info(f"âœ… {model_name} æµ‹è¯•å®Œæˆ - {response_time:.2f}s, ${performance_metrics['cost_estimate']['estimated_cost']:.6f}")
            
            return performance_metrics
            
        except Exception as e:
            end_time = time.time()
            logger.error(f"âŒ {model_name} æµ‹è¯•å¤±è´¥: {e}")
            return {
                'model': model_name,
                'success': False,
                'error': str(e),
                'response_time': end_time - start_time
            }
    
    def _assess_analysis_quality(self, analysis: str) -> float:
        """è¯„ä¼°åˆ†æè´¨é‡ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
        if not analysis:
            return 0.0
        
        quality_score = 0.0
        
        # æ£€æŸ¥å…³é”®VPAæœ¯è¯­
        vpa_terms = ['æˆäº¤é‡', 'é‡ä»·', 'æ”¯æ’‘', 'é˜»åŠ›', 'è¶‹åŠ¿', 'çªç ´', 'èƒŒç¦»']
        found_terms = sum(1 for term in vpa_terms if term in analysis)
        quality_score += (found_terms / len(vpa_terms)) * 30
        
        # æ£€æŸ¥æŠ€æœ¯åˆ†ææœ¯è¯­
        tech_terms = ['MACD', 'RSI', 'MA', 'å‡çº¿', 'å¸ƒæ—', 'æŒ‡æ ‡']
        found_tech = sum(1 for term in tech_terms if term in analysis)
        quality_score += (found_tech / len(tech_terms)) * 20
        
        # æ£€æŸ¥å»ºè®®å’Œç»“è®º
        advice_terms = ['å»ºè®®', 'é£é™©', 'ç›®æ ‡', 'æ­¢æŸ', 'å…¥åœº', 'å‡ºåœº']
        found_advice = sum(1 for term in advice_terms if term in analysis)
        quality_score += (found_advice / len(advice_terms)) * 25
        
        # æ–‡æœ¬é•¿åº¦åˆç†æ€§
        length_score = min(len(analysis) / 1000, 1) * 25  # æœŸæœ›1000å­—ç¬¦ä»¥ä¸Š
        quality_score += length_score
        
        return min(quality_score, 100.0)
    
    def run_comprehensive_test(self):
        """è¿è¡Œå…¨é¢çš„æ¨¡å‹å¯¹æ¯”æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹2025å¹´æœ€æ–°æ¨¡å‹ç»¼åˆæµ‹è¯•...")
        
        # è·å–æµ‹è¯•æ•°æ®
        logger.info("ğŸ“Š è·å–ETHæ°¸ç»­åˆçº¦æµ‹è¯•æ•°æ®...")
        df = self.fetcher.get_ohlcv(symbol='ETH/USDT', limit=30)  # ä½¿ç”¨è¾ƒå°‘æ•°æ®èŠ‚çœæˆæœ¬
        
        if df is None:
            logger.error("âŒ æ— æ³•è·å–æµ‹è¯•æ•°æ®")
            return None
        
        # æ ¼å¼åŒ–ä¸ºCSVæ ¼å¼ï¼ˆæœ€ç»æµï¼‰
        test_data = self.formatter.to_csv_format(df)
        token_estimate = len(test_data.split())
        
        logger.info(f"ğŸ“ æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆï¼Œé¢„ä¼°è¾“å…¥tokens: {token_estimate}")
        
        # è·å–æµ‹è¯•æ¨¡å‹
        model_groups = self.get_test_models()
        
        results = {
            'test_time': datetime.now().isoformat(),
            'data_info': {
                'symbol': 'ETH/USDT',
                'timeframe': '1h',
                'bars': len(df),
                'estimated_tokens': token_estimate
            },
            'model_results': {},
            'summary': {}
        }
        
        # æŒ‰ç»„æµ‹è¯•æ¨¡å‹
        for group_name, models in model_groups.items():
            logger.info(f"\\nğŸ¯ æµ‹è¯•æ¨¡å‹ç»„: {group_name.upper()}")
            results['model_results'][group_name] = []
            
            for model in models:
                if model in Settings.MODELS:
                    result = self.test_model_performance(model, test_data)
                    results['model_results'][group_name].append(result)
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
                    time.sleep(2)
                else:
                    logger.warning(f"âš ï¸ æ¨¡å‹ {model} ä¸åœ¨é…ç½®ä¸­ï¼Œè·³è¿‡")
        
        # ç”Ÿæˆæµ‹è¯•æ€»ç»“
        results['summary'] = self._generate_summary(results['model_results'])
        
        return results
    
    def _generate_summary(self, model_results: dict) -> dict:
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
        summary = {
            'total_models_tested': 0,
            'successful_models': 0,
            'failed_models': 0,
            'best_performance': {},
            'cost_analysis': {},
            'recommendations': []
        }
        
        all_results = []
        for group_results in model_results.values():
            all_results.extend(group_results)
        
        successful_results = [r for r in all_results if r.get('success', False)]
        
        summary['total_models_tested'] = len(all_results)
        summary['successful_models'] = len(successful_results)
        summary['failed_models'] = len(all_results) - len(successful_results)
        
        if successful_results:
            # æœ€ä½³æ€§èƒ½æ¨¡å‹
            fastest = min(successful_results, key=lambda x: x['response_time'])
            cheapest = min(successful_results, key=lambda x: x['cost_estimate']['estimated_cost'])
            highest_quality = max(successful_results, key=lambda x: x['analysis_quality_score'])
            
            summary['best_performance'] = {
                'fastest': {'model': fastest['model'], 'time': fastest['response_time']},
                'cheapest': {'model': cheapest['model'], 'cost': cheapest['cost_estimate']['estimated_cost']},
                'highest_quality': {'model': highest_quality['model'], 'score': highest_quality['analysis_quality_score']}
            }
            
            # æˆæœ¬åˆ†æ
            avg_cost = sum(r['cost_estimate']['estimated_cost'] for r in successful_results) / len(successful_results)
            total_cost = sum(r['cost_estimate']['estimated_cost'] for r in successful_results)
            
            summary['cost_analysis'] = {
                'average_cost_per_query': round(avg_cost, 6),
                'total_test_cost': round(total_cost, 6),
                'cost_range': {
                    'min': round(min(r['cost_estimate']['estimated_cost'] for r in successful_results), 6),
                    'max': round(max(r['cost_estimate']['estimated_cost'] for r in successful_results), 6)
                }
            }
        
        # ç”Ÿæˆå»ºè®®
        if successful_results:
            summary['recommendations'].append(f"âœ… æˆåŠŸæµ‹è¯• {len(successful_results)} ä¸ªæ¨¡å‹")
            summary['recommendations'].append(f"ğŸ’° æœ€ç»æµé€‰æ‹©: {cheapest['model']} (${cheapest['cost_estimate']['estimated_cost']:.6f})")
            summary['recommendations'].append(f"âš¡ æœ€å¿«å“åº”: {fastest['model']} ({fastest['response_time']:.2f}ç§’)")
            summary['recommendations'].append(f"ğŸ¯ æœ€é«˜è´¨é‡: {highest_quality['model']} (è´¨é‡åˆ†: {highest_quality['analysis_quality_score']:.1f})")
        
        return summary
    
    def print_results(self, results: dict):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print("\\n" + "="*80)
        print("ğŸ“Š 2025å¹´æœ€æ–°æ¨¡å‹æµ‹è¯•ç»“æœ")
        print("="*80)
        
        print(f"\\nğŸ• æµ‹è¯•æ—¶é—´: {results['test_time']}")
        print(f"ğŸ“ˆ æµ‹è¯•æ•°æ®: {results['data_info']['symbol']} {results['data_info']['bars']}æ ¹Kçº¿")
        print(f"ğŸ’¡ é¢„ä¼°tokens: {results['data_info']['estimated_tokens']}")
        
        # æŒ‰ç»„æ˜¾ç¤ºç»“æœ
        for group_name, group_results in results['model_results'].items():
            print(f"\\nğŸ¯ {group_name.upper()} æ¨¡å‹ç»„:")
            print("-" * 50)
            
            for result in group_results:
                if result['success']:
                    print(f"  âœ… {result['model']:15} | "
                          f"{result['response_time']:6.2f}s | "
                          f"${result['cost_estimate']['estimated_cost']:8.6f} | "
                          f"è´¨é‡: {result['analysis_quality_score']:5.1f}")
                else:
                    print(f"  âŒ {result['model']:15} | å¤±è´¥: {result.get('error', 'Unknown')}")
        
        # æ˜¾ç¤ºæ€»ç»“
        summary = results['summary']
        print(f"\\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print("-" * 50)
        print(f"æ€»æµ‹è¯•æ¨¡å‹: {summary['total_models_tested']}")
        print(f"æˆåŠŸ: {summary['successful_models']}, å¤±è´¥: {summary['failed_models']}")
        
        if summary['best_performance']:
            print(f"\\nğŸ† æœ€ä½³è¡¨ç°:")
            bp = summary['best_performance']
            print(f"  æœ€å¿«: {bp['fastest']['model']} ({bp['fastest']['time']:.2f}s)")
            print(f"  æœ€ä¾¿å®œ: {bp['cheapest']['model']} (${bp['cheapest']['cost']:.6f})")
            print(f"  æœ€é«˜è´¨é‡: {bp['highest_quality']['model']} ({bp['highest_quality']['score']:.1f}åˆ†)")
        
        if summary['cost_analysis']:
            ca = summary['cost_analysis']
            print(f"\\nğŸ’° æˆæœ¬åˆ†æ:")
            print(f"  å¹³å‡æˆæœ¬: ${ca['average_cost_per_query']:.6f}")
            print(f"  æ€»æµ‹è¯•è´¹ç”¨: ${ca['total_test_cost']:.6f}")
            print(f"  æˆæœ¬èŒƒå›´: ${ca['cost_range']['min']:.6f} - ${ca['cost_range']['max']:.6f}")
        
        print("\\nğŸ’¡ å»ºè®®:")
        for rec in summary['recommendations']:
            print(f"  {rec}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*80)
    print("ğŸ§ª 2025å¹´æœ€æ–°LLMæ¨¡å‹é‡ä»·åˆ†æèƒ½åŠ›æµ‹è¯•")
    print("="*80)
    
    try:
        tester = Model2025Tester()
        results = tester.run_comprehensive_test()
        
        if results:
            tester.print_results(results)
            
            print("\\nğŸ‰ æµ‹è¯•å®Œæˆï¼å»ºè®®åŸºäºä»¥ä¸Šç»“æœé€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹è¿›è¡Œé‡ä»·åˆ†æã€‚")
        else:
            print("\\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚")
            
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        print(f"\\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()