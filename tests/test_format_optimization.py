#!/usr/bin/env python3
"""
é˜¶æ®µ2: æ•°æ®æ ¼å¼ä¼˜åŒ–æµ‹è¯•
ç³»ç»Ÿæ€§æ¯”è¾ƒ4ç§æ•°æ®æ ¼å¼åœ¨VPAåˆ†æè´¨é‡ä¸Šçš„æ•ˆæœ
"""

import os
import sys
import asyncio
import time
import json
import pandas as pd
from typing import Dict, List, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data import BinanceFetcher
from formatters import DataFormatter
from ai import OpenRouterClient
from config import Settings

class FormatOptimizationTest:
    """æ•°æ®æ ¼å¼ä¼˜åŒ–æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.fetcher = BinanceFetcher()
        self.formatter = DataFormatter()
        self.client = OpenRouterClient()
        
        # æµ‹è¯•é…ç½®
        self.test_symbol = 'ETH/USDT'
        self.test_timeframe = '1h'
        self.test_limit = 50  # Kçº¿æ•°é‡
        
        # æµ‹è¯•æ¨¡å‹ï¼ˆé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„æ¨¡å‹ï¼‰
        self.test_models = [
            'gpt5-mini',        # GPT-5 Mini (æ¨ç†ç‰ˆ)
            'gpt4o-mini',       # GPT-4o mini (ç»æµç‰ˆ)
            'claude-haiku',     # Claude Haiku (è½»é‡ç‰ˆ)
            'gemini-flash',     # Gemini Flash (å¿«é€Ÿç‰ˆ)
        ]
        
        # æ•°æ®æ ¼å¼
        self.formats = ['csv', 'text', 'json', 'pattern']
        
    def run_format_comparison(self) -> Dict[str, Any]:
        """è¿è¡Œæ ¼å¼å¯¹æ¯”æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹é˜¶æ®µ2: æ•°æ®æ ¼å¼ä¼˜åŒ–æµ‹è¯•")
        print(f"ğŸ“Š æµ‹è¯•é…ç½®: {self.test_symbol} {self.test_timeframe} {self.test_limit}æ¡Kçº¿")
        print(f"ğŸ¤– æµ‹è¯•æ¨¡å‹: {', '.join(self.test_models)}")
        print(f"ğŸ“ æµ‹è¯•æ ¼å¼: {', '.join(self.formats)}")
        print("="*80)
        
        # è·å–æµ‹è¯•æ•°æ®
        print("ğŸ“Š è·å–å¸‚åœºæ•°æ®...")
        df = self.fetcher.get_ohlcv(
            symbol=self.test_symbol,
            timeframe=self.test_timeframe,
            limit=self.test_limit
        )
        
        # ç”Ÿæˆæ‰€æœ‰æ ¼å¼çš„æ•°æ®
        format_data = self._generate_format_data(df)
        
        # è¿è¡Œæµ‹è¯•çŸ©é˜µ
        results = self._run_test_matrix(format_data)
        
        # åˆ†æç»“æœ
        analysis = self._analyze_results(results)
        
        # ä¿å­˜ç»“æœ
        self._save_results(results, analysis)
        
        return {
            'raw_results': results,
            'analysis': analysis,
            'test_config': {
                'symbol': self.test_symbol,
                'timeframe': self.test_timeframe,
                'limit': self.test_limit,
                'models': self.test_models,
                'formats': self.formats
            }
        }
    
    def _generate_format_data(self, df: pd.DataFrame) -> Dict[str, str]:
        """ç”Ÿæˆæ‰€æœ‰æ ¼å¼çš„æ•°æ®"""
        print("ğŸ“ ç”Ÿæˆ4ç§æ ¼å¼æ•°æ®...")
        
        format_data = {}
        token_estimates = self.formatter.estimate_tokens_by_format(df)
        
        # CSVæ ¼å¼ (æœ€çœtoken)
        format_data['csv'] = self.formatter.to_csv_format(df)
        print(f"  ğŸ“Š CSVæ ¼å¼: {token_estimates['csv']} tokens")
        
        # æ–‡æœ¬æ ¼å¼ (æœ€æ˜“ç†è§£)
        format_data['text'] = self.formatter.to_text_narrative(df)
        print(f"  ğŸ“– Textæ ¼å¼: {token_estimates['text']} tokens")
        
        # JSONæ ¼å¼ (æœ€è¯¦ç»†)
        format_data['json'] = self.formatter.to_structured_json(df)
        print(f"  ğŸ—‚ï¸ JSONæ ¼å¼: {token_estimates['json']} tokens")
        
        # Patternæ ¼å¼ (æœ€ä¸“ä¸š)
        format_data['pattern'] = self.formatter.to_pattern_description(df)
        print(f"  ğŸ¯ Patternæ ¼å¼: {token_estimates['pattern']} tokens")
        
        return format_data
    
    def _run_test_matrix(self, format_data: Dict[str, str]) -> Dict[str, Dict[str, Any]]:
        """è¿è¡Œæµ‹è¯•çŸ©é˜µ - æ¯ç§æ ¼å¼ x æ¯ä¸ªæ¨¡å‹"""
        results = {}
        total_tests = len(self.formats) * len(self.test_models)
        current_test = 0
        
        for format_name in self.formats:
            results[format_name] = {}
            
            for model_name in self.test_models:
                current_test += 1
                print(f"ğŸ”„ æµ‹è¯• {current_test}/{total_tests}: {format_name} x {model_name}")
                
                try:
                    # è¿è¡ŒVPAåˆ†æ
                    result = self.client.analyze_market_data(
                        data=format_data[format_name],
                        model_name=model_name,
                        analysis_type='vpa'
                    )
                    
                    if result.get('error'):
                        print(f"  âŒ å¤±è´¥: {result['error']}")
                        results[format_name][model_name] = {
                            'error': result['error'],
                            'success': False
                        }
                    else:
                        # è®¡ç®—è´¨é‡è¯„åˆ†
                        quality_score = self._calculate_quality_score(result['analysis'])
                        
                        results[format_name][model_name] = {
                            'success': True,
                            'analysis': result['analysis'],
                            'usage': result.get('usage', {}),
                            'response_time': result.get('response_time', 0),
                            'quality_score': quality_score,
                            'model_id': result.get('model_id')
                        }
                        
                        print(f"  âœ… æˆåŠŸ: è´¨é‡è¯„åˆ† {quality_score:.1f}/100, "
                              f"è€—æ—¶ {result.get('response_time', 0):.1f}s, "
                              f"tokens {result.get('usage', {}).get('total_tokens', 0)}")
                    
                    # é˜²æ­¢é¢‘ç‡é™åˆ¶
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"  âŒ å¼‚å¸¸: {e}")
                    results[format_name][model_name] = {
                        'error': str(e),
                        'success': False
                    }
        
        return results
    
    def _calculate_quality_score(self, analysis: str) -> float:
        """è®¡ç®—VPAåˆ†æè´¨é‡è¯„åˆ†"""
        score = 0.0
        analysis_lower = analysis.lower()
        
        # VPAä¸“ä¸šæœ¯è¯­æ£€æŸ¥ (30åˆ†)
        vpa_terms = [
            'é‡ä»·', 'æˆäº¤é‡', 'volume', 'accumulation', 'distribution',
            'markup', 'markdown', 'èƒŒç¦»', 'å¼‚å¸¸æˆäº¤é‡', 'ä¸»åŠ›', 'èµ„é‡‘æµ'
        ]
        term_score = sum(1 for term in vpa_terms if term in analysis_lower)
        score += min(term_score * 3, 30)
        
        # å¸‚åœºé˜¶æ®µåˆ¤æ–­ (25åˆ†)
        stage_terms = ['accumulation', 'distribution', 'markup', 'markdown', 'å¸ç­¹', 'æ´¾å‘', 'æ‹‰å‡', 'ä¸‹è·Œ']
        if any(term in analysis_lower for term in stage_terms):
            score += 25
        
        # å…·ä½“æ•°æ®å¼•ç”¨ (20åˆ†)
        if any(indicator in analysis for indicator in ['ä»·æ ¼', 'æˆäº¤é‡', 'ohlc', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½']):
            score += 20
        
        # äº¤æ˜“ä¿¡å·å’Œå»ºè®® (15åˆ†)
        signal_terms = ['å»ºè®®', 'ä¹°å…¥', 'å–å‡º', 'æ­¢æŸ', 'ç›®æ ‡', 'æ”¯æ’‘', 'é˜»åŠ›', 'çªç ´']
        if any(term in analysis_lower for term in signal_terms):
            score += 15
        
        # é£é™©è¯„ä¼° (10åˆ†)
        risk_terms = ['é£é™©', 'è°¨æ…', 'æ³¨æ„', 'è­¦å‘Š', 'risk', 'caution']
        if any(term in analysis_lower for term in risk_terms):
            score += 10
        
        return min(score, 100.0)
    
    def _analyze_results(self, results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        print("\nğŸ“Š åˆ†ææµ‹è¯•ç»“æœ...")
        
        analysis = {
            'format_performance': {},
            'model_performance': {},
            'cost_effectiveness': {},
            'recommendations': []
        }
        
        # æŒ‰æ ¼å¼åˆ†æ
        for format_name in self.formats:
            format_results = results.get(format_name, {})
            successful_results = [r for r in format_results.values() if r.get('success')]
            
            if successful_results:
                avg_quality = sum(r['quality_score'] for r in successful_results) / len(successful_results)
                avg_time = sum(r['response_time'] for r in successful_results) / len(successful_results)
                avg_tokens = sum(r['usage'].get('total_tokens', 0) for r in successful_results) / len(successful_results)
                
                analysis['format_performance'][format_name] = {
                    'avg_quality_score': round(avg_quality, 2),
                    'avg_response_time': round(avg_time, 2),
                    'avg_tokens': round(avg_tokens, 0),
                    'success_rate': len(successful_results) / len(self.test_models)
                }
        
        # æŒ‰æ¨¡å‹åˆ†æ
        for model_name in self.test_models:
            model_results = []
            for format_name in self.formats:
                result = results.get(format_name, {}).get(model_name, {})
                if result.get('success'):
                    model_results.append(result)
            
            if model_results:
                avg_quality = sum(r['quality_score'] for r in model_results) / len(model_results)
                avg_time = sum(r['response_time'] for r in model_results) / len(model_results)
                
                analysis['model_performance'][model_name] = {
                    'avg_quality_score': round(avg_quality, 2),
                    'avg_response_time': round(avg_time, 2),
                    'success_rate': len(model_results) / len(self.formats)
                }
        
        # ç”Ÿæˆæ¨è
        self._generate_recommendations(analysis)
        
        return analysis
    
    def _generate_recommendations(self, analysis: Dict[str, Any]):
        """ç”Ÿæˆä¼˜åŒ–æ¨è"""
        recommendations = []
        
        # æ‰¾å‡ºæœ€ä½³æ ¼å¼
        format_perf = analysis['format_performance']
        if format_perf:
            best_quality_format = max(format_perf.keys(), key=lambda x: format_perf[x]['avg_quality_score'])
            best_speed_format = min(format_perf.keys(), key=lambda x: format_perf[x]['avg_response_time'])
            best_token_format = min(format_perf.keys(), key=lambda x: format_perf[x]['avg_tokens'])
            
            recommendations.append(f"ğŸ† æœ€ä½³åˆ†æè´¨é‡æ ¼å¼: {best_quality_format} ({format_perf[best_quality_format]['avg_quality_score']:.1f}/100)")
            recommendations.append(f"âš¡ æœ€å¿«å“åº”æ ¼å¼: {best_speed_format} ({format_perf[best_speed_format]['avg_response_time']:.1f}s)")
            recommendations.append(f"ğŸ’° æœ€çœTokenæ ¼å¼: {best_token_format} ({format_perf[best_token_format]['avg_tokens']:.0f} tokens)")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        model_perf = analysis['model_performance']
        if model_perf:
            best_model = max(model_perf.keys(), key=lambda x: model_perf[x]['avg_quality_score'])
            recommendations.append(f"ğŸ¤– æœ€ä½³åˆ†ææ¨¡å‹: {best_model} ({model_perf[best_model]['avg_quality_score']:.1f}/100)")
        
        analysis['recommendations'] = recommendations
    
    def _save_results(self, results: Dict[str, Any], analysis: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = f"results/format_optimization_test_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': timestamp,
                'test_config': {
                    'symbol': self.test_symbol,
                    'timeframe': self.test_timeframe,
                    'limit': self.test_limit,
                    'models': self.test_models,
                    'formats': self.formats
                },
                'results': results,
                'analysis': analysis
            }, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {results_file}")
        
        # æ‰“å°å…³é”®ç»“æœ
        self._print_summary(analysis)
    
    def _print_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*80)
        print("ğŸ“Š é˜¶æ®µ2æ•°æ®æ ¼å¼ä¼˜åŒ–æµ‹è¯•ç»“æœæ€»ç»“")
        print("="*80)
        
        # æ ¼å¼æ€§èƒ½å¯¹æ¯”
        print("\nğŸ¯ å„æ ¼å¼æ€§èƒ½å¯¹æ¯”:")
        format_perf = analysis['format_performance']
        for format_name, perf in format_perf.items():
            print(f"  {format_name:>8}: è´¨é‡ {perf['avg_quality_score']:>5.1f}/100 | "
                  f"é€Ÿåº¦ {perf['avg_response_time']:>5.1f}s | "
                  f"Token {perf['avg_tokens']:>6.0f} | "
                  f"æˆåŠŸç‡ {perf['success_rate']*100:>5.1f}%")
        
        # æ¨¡å‹æ€§èƒ½å¯¹æ¯”
        print("\nğŸ¤– å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        model_perf = analysis['model_performance']
        for model_name, perf in model_perf.items():
            print(f"  {model_name:>15}: è´¨é‡ {perf['avg_quality_score']:>5.1f}/100 | "
                  f"é€Ÿåº¦ {perf['avg_response_time']:>5.1f}s | "
                  f"æˆåŠŸç‡ {perf['success_rate']*100:>5.1f}%")
        
        # æ¨è
        print("\nğŸ’¡ ä¼˜åŒ–æ¨è:")
        for rec in analysis['recommendations']:
            print(f"  {rec}")
        
        print("\n" + "="*80)

def main():
    """ä¸»å‡½æ•°"""
    try:
        # éªŒè¯é…ç½®
        Settings.validate()
        
        # åˆ›å»ºæµ‹è¯•å®ä¾‹
        test = FormatOptimizationTest()
        
        # è¿è¡Œæµ‹è¯•
        results = test.run_format_comparison()
        
        print("âœ… é˜¶æ®µ2æ•°æ®æ ¼å¼ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())