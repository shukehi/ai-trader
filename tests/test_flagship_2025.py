#!/usr/bin/env python3
"""
2025å¹´æ——èˆ°æ¨¡å‹ä¸“é¡¹æµ‹è¯•
ä¸“é—¨æµ‹è¯•GPT-5ã€Claude Opus 4.1ã€Gemini 2.5 Proã€Grok-4çš„é‡ä»·åˆ†æèƒ½åŠ›
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from datetime import datetime
from data import BinanceFetcher, DataProcessor
from formatters import DataFormatter
from ai import OpenRouterClient
from config import Settings
import logging
import time
import json

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FlagshipModelTester:
    """2025å¹´æ——èˆ°æ¨¡å‹æµ‹è¯•å™¨"""
    
    def __init__(self):
        try:
            Settings.validate()
            self.fetcher = BinanceFetcher()
            self.formatter = DataFormatter()
            self.client = OpenRouterClient()
            
            # 2025æ——èˆ°æ¨¡å‹åˆ—è¡¨
            self.flagship_models = {
                'gpt5': {
                    'name': 'GPT-5',
                    'provider': 'OpenAI',
                    'speciality': 'é€šç”¨æ¨ç†å’Œåˆ†æ',
                    'context': '1M tokens'
                },
                'claude-opus-41': {
                    'name': 'Claude Opus 4.1',
                    'provider': 'Anthropic',
                    'speciality': 'æ·±åº¦åˆ†æå’Œæ¨ç†',
                    'context': '500K tokens'
                },
                'gemini-25-pro': {
                    'name': 'Gemini 2.5 Pro',
                    'provider': 'Google',
                    'speciality': 'å¤šæ¨¡æ€å’Œé•¿ä¸Šä¸‹æ–‡',
                    'context': '10M tokens'
                },
                'grok4': {
                    'name': 'Grok-4',
                    'provider': 'xAI',
                    'speciality': 'å®æ—¶ä¿¡æ¯å’Œåˆ›æ–°æ€è€ƒ',
                    'context': '1M tokens'
                }
            }
            
            logger.info("ğŸš€ 2025æ——èˆ°æ¨¡å‹æµ‹è¯•å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def test_vpa_analysis_depth(self, model_name: str, data: str) -> dict:
        """æµ‹è¯•VPAåˆ†ææ·±åº¦å’Œä¸“ä¸šæ€§"""
        logger.info(f"ğŸ§ª æµ‹è¯• {model_name} çš„VPAåˆ†ææ·±åº¦...")
        
        # ä¸“é—¨çš„VPAæ·±åº¦åˆ†ææç¤º
        vpa_expert_prompt = """
ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„VPA(Volume Price Analysis)ä¸“å®¶ï¼Œæ·±è°™Anna Coullingçš„ç†è®ºä½“ç³»ã€‚
è¯·å¯¹æä¾›çš„ETHæ°¸ç»­åˆçº¦æ•°æ®è¿›è¡Œæœ€ä¸“ä¸šæ·±å…¥çš„VPAåˆ†æï¼š

æ ¸å¿ƒåˆ†æè¦æ±‚ï¼š
1. ã€å¸‚åœºé˜¶æ®µåˆ¤æ–­ã€‘- å½“å‰å¤„äºAccumulation, Distribution, Markupè¿˜æ˜¯Markdowné˜¶æ®µï¼Ÿ
2. ã€é‡ä»·å…³ç³»éªŒè¯ã€‘- æ¯ä¸ªé‡è¦ä»·æ ¼å˜åŒ–æ˜¯å¦å¾—åˆ°æˆäº¤é‡ç¡®è®¤ï¼Ÿè¯†åˆ«é‡ä»·èƒŒç¦»
3. ã€ä¸“ä¸šèµ„é‡‘è¡Œä¸ºã€‘- åˆ†æå¤§èµ„é‡‘çš„è¿›å‡ºç—•è¿¹ï¼Œè¯†åˆ«Smart Money vs Dumb Money
4. ã€å…³é”®VPAä¿¡å·ã€‘- è¯†åˆ«Stopping Volume, Testing Volume, Climax Volumeç­‰
5. ã€æ°¸ç»­åˆçº¦ç‰¹è‰²ã€‘- ç»“åˆèµ„é‡‘è´¹ç‡å˜åŒ–åˆ†æå¸‚åœºæƒ…ç»ªå’ŒæŒä»“ç»“æ„

åˆ†ææ¡†æ¶ï¼š
- ä½¿ç”¨Wyckoffç†è®ºçš„Supply/Demandåˆ†æ
- è¯†åˆ«Cause and EffectåŸç†åœ¨ä»·æ ¼è¡Œä¸ºä¸­çš„ä½“ç°
- åˆ¤æ–­å½“å‰æ˜¯å¦å­˜åœ¨No Demandæˆ–No Supplyçš„è¿¹è±¡
- åˆ†æVolume Spread Analysis (VSA) çš„æ ¸å¿ƒåŸç†

è¯·æä¾›ï¼š
1. å¸‚åœºå½“å‰é˜¶æ®µçš„æ˜ç¡®åˆ¤æ–­ï¼ˆå«ç½®ä¿¡åº¦ï¼‰
2. æœ€å…³é”®çš„3ä¸ªVPAä¿¡å·åŠå…¶äº¤æ˜“å«ä¹‰
3. åŸºäºVPAçš„å…·ä½“æ“ä½œå»ºè®®ï¼ˆè¿›åœºä½ã€æ­¢æŸä½ã€ç›®æ ‡ä½ï¼‰
4. é£é™©è¯„ä¼°å’Œä»“ä½ç®¡ç†å»ºè®®

è¦æ±‚ä¸“ä¸šæœ¯è¯­å‡†ç¡®ï¼Œé€»è¾‘æ¸…æ™°ï¼Œå¯æ“ä½œæ€§å¼ºã€‚
        """.strip()
        
        try:
            start_time = time.time()
            
            response = self.client.client.chat.completions.create(
                model=Settings.MODELS[model_name],
                messages=[
                    {"role": "system", "content": vpa_expert_prompt},
                    {"role": "user", "content": data}
                ],
                temperature=0.1,
                max_tokens=4000,
            )
            
            end_time = time.time()
            
            analysis = response.choices[0].message.content
            usage = response.usage
            
            # åˆ†æè´¨é‡è¯„åˆ†
            quality_score = self._evaluate_vpa_analysis_quality(analysis)
            
            return {
                'model': model_name,
                'success': True,
                'analysis': analysis,
                'response_time': end_time - start_time,
                'token_usage': {
                    'prompt_tokens': usage.prompt_tokens,
                    'completion_tokens': usage.completion_tokens,
                    'total_tokens': usage.total_tokens
                },
                'quality_metrics': quality_score,
                'cost_estimate': self.client.estimate_cost(
                    model_name, usage.prompt_tokens, usage.completion_tokens
                )
            }
            
        except Exception as e:
            logger.error(f"âŒ {model_name} VPAæµ‹è¯•å¤±è´¥: {e}")
            return {
                'model': model_name,
                'success': False,
                'error': str(e),
                'response_time': time.time() - start_time
            }
    
    def _evaluate_vpa_analysis_quality(self, analysis: str) -> dict:
        """è¯„ä¼°VPAåˆ†æè´¨é‡"""
        scores = {
            'vpa_terminology': 0,      # VPAä¸“ä¸šæœ¯è¯­ä½¿ç”¨
            'market_stage_clarity': 0,  # å¸‚åœºé˜¶æ®µåˆ¤æ–­æ¸…æ™°åº¦
            'actionable_advice': 0,     # å¯æ“ä½œçš„å»ºè®®
            'risk_management': 0,       # é£é™©ç®¡ç†è¦ç´ 
            'professional_depth': 0,    # ä¸“ä¸šæ·±åº¦
            'total_score': 0
        }
        
        if not analysis:
            return scores
        
        analysis_lower = analysis.lower()
        
        # VPAä¸“ä¸šæœ¯è¯­ (30åˆ†)
        vpa_terms = [
            'accumulation', 'distribution', 'markup', 'markdown',
            'stopping volume', 'testing volume', 'climax',
            'supply', 'demand', 'wyckoff', 'smart money',
            'volume spread', 'no demand', 'no supply'
        ]
        found_vpa = sum(1 for term in vpa_terms if term in analysis_lower)
        scores['vpa_terminology'] = min(found_vpa / len(vpa_terms) * 30, 30)
        
        # å¸‚åœºé˜¶æ®µåˆ¤æ–­ (25åˆ†)
        stage_indicators = ['é˜¶æ®µ', 'å½“å‰', 'åˆ¤æ–­', 'ç½®ä¿¡åº¦', 'phase']
        found_stage = sum(1 for term in stage_indicators if term in analysis_lower)
        scores['market_stage_clarity'] = min(found_stage / len(stage_indicators) * 25, 25)
        
        # å¯æ“ä½œå»ºè®® (25åˆ†)
        action_terms = ['å»ºè®®', 'è¿›åœº', 'æ­¢æŸ', 'ç›®æ ‡', 'æ“ä½œ', 'ä¹°å…¥', 'å–å‡º']
        found_action = sum(1 for term in action_terms if term in analysis)
        scores['actionable_advice'] = min(found_action / len(action_terms) * 25, 25)
        
        # é£é™©ç®¡ç† (10åˆ†)
        risk_terms = ['é£é™©', 'ä»“ä½', 'ç®¡ç†', 'risk', 'æ­¢æŸ']
        found_risk = sum(1 for term in risk_terms if term in analysis)
        scores['risk_management'] = min(found_risk / len(risk_terms) * 10, 10)
        
        # ä¸“ä¸šæ·±åº¦ (10åˆ†) - åŸºäºåˆ†æé•¿åº¦å’Œç»“æ„
        if len(analysis) > 1000:
            scores['professional_depth'] += 5
        if '1.' in analysis or '2.' in analysis:  # ç»“æ„åŒ–åˆ†æ
            scores['professional_depth'] += 5
        
        scores['total_score'] = sum(scores.values()) - scores['total_score']  # æ’é™¤total_scoreè‡ªèº«
        
        return scores
    
    def run_flagship_comparison(self, limit: int = 30):
        """è¿è¡Œ2025æ——èˆ°æ¨¡å‹å¯¹æ¯”æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹2025æ——èˆ°æ¨¡å‹VPAåˆ†æå¯¹æ¯”æµ‹è¯•...")
        
        # è·å–æµ‹è¯•æ•°æ®
        logger.info("ğŸ“Š è·å–ETHæ°¸ç»­åˆçº¦æ•°æ®...")
        df = self.fetcher.get_ohlcv(symbol='ETH/USDT', limit=limit)
        
        if df is None:
            logger.error("âŒ æ— æ³•è·å–æµ‹è¯•æ•°æ®")
            return None
        
        # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ä»¥æä¾›æ›´ä¸°å¯Œçš„åˆ†ææ•°æ®
        df_with_indicators = DataProcessor.add_basic_indicators(df)
        df_with_vpa = DataProcessor.analyze_volume_price_relationship(df_with_indicators)
        
        # æ ¼å¼åŒ–ä¸ºVPAä¸“ç”¨æ•°æ®æ ¼å¼
        test_data = self._prepare_vpa_test_data(df_with_vpa)
        
        logger.info(f"ğŸ“ VPAæµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆï¼Œæ•°æ®é‡: {len(df)}æ ¹Kçº¿")
        
        results = {
            'test_info': {
                'timestamp': datetime.now().isoformat(),
                'symbol': 'ETH/USDT',
                'timeframe': '1h',
                'data_points': len(df),
                'test_type': 'VPA Professional Analysis'
            },
            'flagship_results': {},
            'comparison': {}
        }
        
        # æµ‹è¯•æ¯ä¸ªæ——èˆ°æ¨¡å‹
        for model_key, model_info in self.flagship_models.items():
            if model_key in Settings.MODELS:
                logger.info(f"\\nğŸ¯ æµ‹è¯• {model_info['name']} ({model_info['provider']})...")
                
                result = self.test_vpa_analysis_depth(model_key, test_data)
                results['flagship_results'][model_key] = {
                    'model_info': model_info,
                    'test_result': result
                }
                
                # é˜²æ­¢APIé¢‘ç‡é™åˆ¶
                time.sleep(3)
            else:
                logger.warning(f"âš ï¸ æ¨¡å‹ {model_key} æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°")
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        results['comparison'] = self._generate_flagship_comparison(results['flagship_results'])
        
        return results
    
    def _prepare_vpa_test_data(self, df) -> str:
        """å‡†å¤‡VPAä¸“ç”¨æµ‹è¯•æ•°æ®æ ¼å¼"""
        lines = [f"# ETH/USDT æ°¸ç»­åˆçº¦ VPA ä¸“ä¸šåˆ†ææ•°æ®\\n"]
        
        # æ•°æ®æ¦‚è§ˆ
        lines.append("## å¸‚åœºæ¦‚å†µ")
        lines.append(f"æ—¶é—´èŒƒå›´: {df['datetime'].iloc[0]} è‡³ {df['datetime'].iloc[-1]}")
        lines.append(f"ä»·æ ¼å˜åŒ–: {df['close'].iloc[0]:.2f} â†’ {df['close'].iloc[-1]:.2f} ({((df['close'].iloc[-1]/df['close'].iloc[0]-1)*100):+.2f}%)")
        lines.append(f"æˆäº¤é‡æ€»è®¡: {df['volume'].sum():,.0f}")
        lines.append("")
        
        # è¯¦ç»†Kçº¿æ•°æ®å«æŠ€æœ¯æŒ‡æ ‡
        lines.append("## Kçº¿æ•°æ® (å«VPAå…³é”®æŒ‡æ ‡)")
        header = "æ—¶é—´,å¼€ç›˜,æœ€é«˜,æœ€ä½,æ”¶ç›˜,æˆäº¤é‡,é‡æ¯”,RSI,MACD,å¸ƒæ—ä¸Šè½¨,VPAä¿¡å·"
        lines.append(header)
        
        for _, row in df.tail(20).iterrows():  # æœ€è¿‘20æ ¹Kçº¿
            vpa_signals = []
            if row.get('bullish_volume', False):
                vpa_signals.append('å¥åº·ä¸Šæ¶¨')
            if row.get('bearish_volume', False):
                vpa_signals.append('å¥åº·ä¸‹è·Œ')
            if row.get('suspicious_rally', False):
                vpa_signals.append('å¯ç–‘ä¸Šæ¶¨')
            if row.get('high_volume_no_progress', False):
                vpa_signals.append('é«˜é‡æ— è¿›å±•')
            
            vpa_signal_str = ','.join(vpa_signals) if vpa_signals else 'æ­£å¸¸'
            
            line = (f"{row['datetime']},{row['open']:.2f},{row['high']:.2f},"
                   f"{row['low']:.2f},{row['close']:.2f},{row['volume']:.0f},"
                   f"{row.get('volume_ratio', 1.0):.2f},"
                   f"{row.get('rsi', 0):.1f},{row.get('macd', 0):.4f},"
                   f"{row.get('bb_upper', row['close']):.2f},{vpa_signal_str}")
            lines.append(line)
        
        return "\\n".join(lines)
    
    def _generate_flagship_comparison(self, results: dict) -> dict:
        """ç”Ÿæˆæ——èˆ°æ¨¡å‹å¯¹æ¯”åˆ†æ"""
        comparison = {
            'performance_ranking': {},
            'cost_analysis': {},
            'quality_analysis': {},
            'recommendations': {}
        }
        
        successful_results = {k: v for k, v in results.items() 
                            if v['test_result'].get('success', False)}
        
        if not successful_results:
            return comparison
        
        # æ€§èƒ½æ’å
        by_speed = sorted(successful_results.items(), 
                         key=lambda x: x[1]['test_result']['response_time'])
        by_cost = sorted(successful_results.items(), 
                        key=lambda x: x[1]['test_result']['cost_estimate']['estimated_cost'])
        by_quality = sorted(successful_results.items(), 
                          key=lambda x: x[1]['test_result']['quality_metrics']['total_score'], 
                          reverse=True)
        
        comparison['performance_ranking'] = {
            'fastest': {
                'model': by_speed[0][0],
                'name': by_speed[0][1]['model_info']['name'],
                'time': by_speed[0][1]['test_result']['response_time']
            },
            'most_economical': {
                'model': by_cost[0][0], 
                'name': by_cost[0][1]['model_info']['name'],
                'cost': by_cost[0][1]['test_result']['cost_estimate']['estimated_cost']
            },
            'highest_quality': {
                'model': by_quality[0][0],
                'name': by_quality[0][1]['model_info']['name'], 
                'score': by_quality[0][1]['test_result']['quality_metrics']['total_score']
            }
        }
        
        # æˆæœ¬åˆ†æ
        total_cost = sum(r['test_result']['cost_estimate']['estimated_cost'] 
                        for r in successful_results.values())
        avg_cost = total_cost / len(successful_results)
        
        comparison['cost_analysis'] = {
            'total_test_cost': round(total_cost, 6),
            'average_cost': round(avg_cost, 6),
            'cost_range': {
                'min': by_cost[0][1]['test_result']['cost_estimate']['estimated_cost'],
                'max': by_cost[-1][1]['test_result']['cost_estimate']['estimated_cost']
            }
        }
        
        return comparison
    
    def print_flagship_results(self, results: dict):
        """æ‰“å°æ——èˆ°æ¨¡å‹æµ‹è¯•ç»“æœ"""
        print("\\n" + "="*100)
        print("ğŸ”¥ 2025å¹´æ——èˆ°æ¨¡å‹VPAåˆ†æèƒ½åŠ›å¯¹æ¯”æµ‹è¯•ç»“æœ")
        print("="*100)
        
        test_info = results['test_info']
        print(f"\\nğŸ“Š æµ‹è¯•ä¿¡æ¯:")
        print(f"  æ—¶é—´: {test_info['timestamp']}")
        print(f"  æ ‡çš„: {test_info['symbol']}")
        print(f"  æ•°æ®: {test_info['data_points']}æ ¹Kçº¿")
        print(f"  ç±»å‹: {test_info['test_type']}")
        
        # å„æ¨¡å‹è¯¦ç»†ç»“æœ
        print(f"\\nğŸ¯ æ¨¡å‹æµ‹è¯•ç»“æœ:")
        print("-" * 100)
        
        for model_key, result_data in results['flagship_results'].items():
            model_info = result_data['model_info']
            test_result = result_data['test_result']
            
            print(f"\\nğŸ¤– {model_info['name']} ({model_info['provider']})")
            print(f"   ç‰¹é•¿: {model_info['speciality']}")
            print(f"   ä¸Šä¸‹æ–‡: {model_info['context']}")
            
            if test_result.get('success'):
                quality = test_result['quality_metrics']
                cost = test_result['cost_estimate']
                
                print(f"   âœ… çŠ¶æ€: æˆåŠŸ")
                print(f"   â±ï¸  å“åº”æ—¶é—´: {test_result['response_time']:.2f}ç§’")
                print(f"   ğŸ’° æˆæœ¬: ${cost['estimated_cost']:.6f}")
                print(f"   ğŸ“Š è´¨é‡è¯„åˆ†: {quality['total_score']:.1f}/100")
                print(f"   ğŸ¯ VPAä¸“ä¸šåº¦: {quality['vpa_terminology']:.1f}/30")
                print(f"   ğŸ“ˆ é˜¶æ®µåˆ¤æ–­: {quality['market_stage_clarity']:.1f}/25")
                print(f"   ğŸ’¡ å¯æ“ä½œæ€§: {quality['actionable_advice']:.1f}/25")
                
                # æ˜¾ç¤ºåˆ†æé¢„è§ˆ
                analysis_preview = test_result['analysis'][:300] + "..."
                print(f"   ğŸ“ åˆ†æé¢„è§ˆ: {analysis_preview}")
                
            else:
                print(f"   âŒ çŠ¶æ€: å¤±è´¥ - {test_result.get('error', 'Unknown error')}")
        
        # å¯¹æ¯”æ€»ç»“
        if results['comparison']:
            comp = results['comparison']
            print(f"\\nğŸ† ç»¼åˆå¯¹æ¯”:")
            print("-" * 50)
            
            if 'performance_ranking' in comp:
                pr = comp['performance_ranking']
                print(f"ğŸš€ æœ€å¿«å“åº”: {pr['fastest']['name']} ({pr['fastest']['time']:.2f}ç§’)")
                print(f"ğŸ’° æœ€ç»æµ: {pr['most_economical']['name']} (${pr['most_economical']['cost']:.6f})")
                print(f"ğŸ¯ æœ€é«˜è´¨é‡: {pr['highest_quality']['name']} ({pr['highest_quality']['score']:.1f}åˆ†)")
            
            if 'cost_analysis' in comp:
                ca = comp['cost_analysis']
                print(f"\\nğŸ’¸ æˆæœ¬åˆ†æ:")
                print(f"  æ€»æµ‹è¯•è´¹ç”¨: ${ca['total_test_cost']:.6f}")
                print(f"  å¹³å‡è´¹ç”¨: ${ca['average_cost']:.6f}")
                print(f"  è´¹ç”¨åŒºé—´: ${ca['cost_range']['min']:.6f} - ${ca['cost_range']['max']:.6f}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*100)
    print("ğŸ”¥ 2025å¹´AIæ——èˆ°æ¨¡å‹é‡ä»·åˆ†æä¸“ä¸šèƒ½åŠ›æµ‹è¯•")
    print("ğŸ¯ æµ‹è¯•æ¨¡å‹: GPT-5 | Claude Opus 4.1 | Gemini 2.5 Pro | Grok-4")
    print("="*100)
    
    try:
        tester = FlagshipModelTester()
        results = tester.run_flagship_comparison(limit=30)
        
        if results:
            tester.print_flagship_results(results)
            
            # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"flagship_test_results_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"\\nğŸ’¾ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            print("\\nğŸ‰ 2025æ——èˆ°æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
            
        else:
            print("\\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚")
            
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        print(f"\\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()